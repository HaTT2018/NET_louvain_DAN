{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class traffic_convnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(traffic_convnet, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.num_links = channels\n",
    "        \n",
    "        # Input: [BatchSize, Channels, H, W]\n",
    "        \n",
    "        # define backbone:\n",
    "        self.bn1 = nn.BatchNorm2d(self.channels)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.channels,\n",
    "                out_channels=30,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding_mode='zeros',\n",
    "                padding=1\n",
    "            ),  # ! no padding\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avgpool1 = nn.AvgPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=1\n",
    "        )\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(30)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=30,\n",
    "                out_channels=30,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding_mode='zeros',\n",
    "                padding=1\n",
    "            ),  # ! no padding\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avgpool2 = nn.AvgPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=1\n",
    "        )\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(30)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(5760, self.num_links*6),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1(x)\n",
    "        pad1 = (1, 0, 1, 0)  ###\n",
    "        x = F.pad(x, pad1)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.pad(x, pad1)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        #x = x.view(-1, self.num_links, x.shape[1]//self.num_links)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_L2_dist(total):\n",
    "    try:\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "    except:\n",
    "        total_cpu = total.to('cpu')\n",
    "        len_ = total_cpu.shape[0]\n",
    "        L2_distance = torch.zeros([len_, len_], device=total_cpu.device.type)\n",
    "        for i in range(total_cpu.shape[1]):\n",
    "            total0 = total_cpu[:, i].unsqueeze(0).expand(int(total_cpu.size(0)), int(total_cpu.size(0)))\n",
    "            total1 = total_cpu[:, i].unsqueeze(1).expand(int(total_cpu.size(0)), int(total_cpu.size(0)))\n",
    "            L2_dist = (total0 - total1)**2\n",
    "            L2_distance += L2_dist\n",
    "            \n",
    "    return L2_distance\n",
    "\n",
    "\n",
    "def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    #source = source.cpu()\n",
    "    #target = target.cpu()\n",
    "    n_samples = int(source.size()[0])+int(target.size()[0])  # number of samples\n",
    "    total = torch.cat([source, target], dim=0)\n",
    "    L2_distance = cal_L2_dist(total)\n",
    "                       \n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "    bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "    return sum(kernel_val).to(total.device.type)  #/len(kernel_val)\n",
    "\n",
    "\n",
    "def mmd_rbf_accelerate(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,\n",
    "        kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        s1, s2 = i, (i+1) % batch_size\n",
    "        t1, t2 = s1 + batch_size, s2 + batch_size\n",
    "        loss += kernels[s1, s2] + kernels[t1, t2]\n",
    "        loss -= kernels[s1, t2] + kernels[s2, t1]\n",
    "    return loss / float(batch_size)\n",
    "\n",
    "def mmd_rbf_noaccelerate(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,\n",
    "                              kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    XX = kernels[:batch_size, :batch_size]\n",
    "    YY = kernels[batch_size:, batch_size:]\n",
    "    XY = kernels[:batch_size, batch_size:]\n",
    "    YX = kernels[batch_size:, :batch_size]\n",
    "    loss = torch.mean(XX + YY - XY -YX)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class traffic_dannet(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, if_mmd=False):\n",
    "        super(traffic_dannet, self).__init__()\n",
    "        self.sharedNet = traff_convnet(channels)\n",
    "        self.num_links = channels\n",
    "        self.if_mmd = if_mmd\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        mmd_loss = 0\n",
    "        source_out = self.sharedNet(source)\n",
    "        \n",
    "        if self.if_mmd == True:\n",
    "            target_out = self.sharedNet(target)  # shared net is resnet 50\n",
    "            #loss += mmd.mmd_rbf_accelerate(source, target)\n",
    "            mmd_loss += mmd_rbf_noaccelerate(source_out, target_out)\n",
    "            \n",
    "            source_out = source_out.view(\n",
    "                -1, self.num_links, source_out.shape[1]//self.num_links\n",
    "            )\n",
    "            target_out = target_out.view(\n",
    "                -1, self.num_links, target_out.shape[1]//self.num_links\n",
    "            )\n",
    "\n",
    "            return source_out, target_out, mmd_loss\n",
    "        \n",
    "        else:\n",
    "            source_out = source_out.view(\n",
    "                -1, self.num_links, source_out.shape[1]//self.num_links\n",
    "            )\n",
    "            return source_out, source_out, 0\n",
    "\n",
    "def traff_convnet(channels):\n",
    "    model = traffic_convnet(channels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss_func(preds, labels):\n",
    "    try:\n",
    "        if preds.device.type == 'cuda':\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "        if labels.device.type == 'cuda':\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    mask = labels > 5\n",
    "    return np.mean(np.fabs(labels[mask]-preds[mask])/labels[mask])\n",
    "\n",
    "def smape_loss_func(preds, labels):\n",
    "    try:\n",
    "        if preds.device.type == 'cuda':\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "        if labels.device.type == 'cuda':\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    mask= labels > 5\n",
    "    return np.mean(2*np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\n",
    "\n",
    "def mae_loss_func(preds, labels):\n",
    "    try:\n",
    "        if preds.device.type == 'cuda':\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "        if labels.device.type == 'cuda':\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    mask= labels > 5\n",
    "    return np.fabs((labels[mask]-preds[mask])).mean()\n",
    "\n",
    "def eliminate_nan(b):\n",
    "    a = np.array(b)\n",
    "    c = a[~np.isnan(a)]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(near_road, flow, k, t_p, t_input, t_pre, batch_size, train_prop):\n",
    "    \n",
    "    time_gran = flow.shape[1]//t_p\n",
    "    num_links = near_road.shape[0]\n",
    "    \n",
    "    image = []\n",
    "    for i in range(np.shape(near_road)[0]):\n",
    "        road_id = []\n",
    "        for j in range(k):\n",
    "            # near_road = np.argsort(distance)\n",
    "            # distance[near_road[n]] = nth smallest of \"distance\"\n",
    "            road_id.append(near_road[i][j])\n",
    "        image.append(flow[road_id, :])\n",
    "    image = np.array(image)  # shape = [num_links, k, t_total]\n",
    "    image = np.transpose(image, (1, 2, 0))  # [num_links, k, t_total] --> [k, t_total, num_links]\n",
    "\n",
    "    image3 = []\n",
    "    label = []\n",
    "\n",
    "    # sliding window\n",
    "    for i in range(1, t_p):  # ? interval: [1, 61]\n",
    "        for j in range(time_gran-t_input-t_pre):\n",
    "            image3.append(image[:, i*time_gran+j        :i*time_gran+j+t_input      , :])\n",
    "            label.append(flow  [:, i*time_gran+j+t_input:i*time_gran+j+t_input+t_pre])\n",
    "    \n",
    "    image3 = np.array(image3)\n",
    "    image3 = np.transpose(image3, (0, 3, 1, 2))  # adjusted for pytorch con net\n",
    "    label = np.asarray(label)\n",
    "    \n",
    "\n",
    "    # 划分前80%数据为训练集，最后20%数据为测试集\n",
    "    image_train = image3[:int(np.shape(image3)[0]*train_prop)]\n",
    "    image_val = image3[int(np.shape(image3)[0]*train_prop):]\n",
    "    label_train = label[:int(np.shape(label)[0]*train_prop)]\n",
    "    label_val = label[int(np.shape(label)[0]*train_prop):]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image_train = torch.tensor(image_train, dtype=torch.float32).to(device)\n",
    "    label_train = torch.tensor(label_train, dtype=torch.float32).to(device)\n",
    "    image_val = torch.tensor(image_val, dtype=torch.float32).to(device)\n",
    "    label_val = torch.tensor(label_val, dtype=torch.float32).to(device)\n",
    "    \n",
    "    ###\n",
    "    if image_train.shape[0] != 2956:\n",
    "        temp = torch.cat((image_train, image_train), axis=0)\n",
    "        image_train = torch.cat((temp, temp), axis=0)\n",
    "        temp = torch.cat((label_train, label_train), axis=0)\n",
    "        label_train = torch.cat((temp, temp), axis=0)\n",
    "    ###\n",
    "    \n",
    "    # data loader\n",
    "    #train_dataset = TensorDataset(image_train, label_train)\n",
    "    #train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    return num_links, image_train, label_train, image_val, label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_near_road = np.array(pd.read_csv(\n",
    "    '../data/network/2small_network_nearest_road_id.csv', header=0\n",
    "))\n",
    "\n",
    "tar_near_road = np.array(pd.read_csv('../data/network/small_network_nearest_road_id.csv', header=0))\n",
    "\n",
    "src_flow = np.array(\n",
    "    pd.read_csv('../data/network/2small_network_flow.csv', header=0).iloc[:, 2:]\n",
    ")\n",
    "tar_flow = np.array(pd.read_csv('../data/network/small_network_flow.csv', header=0).iloc[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4 # K个路段\n",
    "t_p = 25  # number of days\n",
    "t_input = 48  # 参数t_input为输入时间窗(5min颗粒度)\n",
    "t_pre = 6  # 参数t_pre为预测时间窗(5min颗粒度)\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 利用滑动窗口的方式，重构数据为(n，最近路段数，输入时间窗，总路段数)的形式\n",
    "src_num_links, src_train_data, src_train_label, src_image_val, src_label_val = sliding_window(\n",
    "    src_near_road, src_flow, k, t_p, t_input, t_pre, batch_size, train_prop=1\n",
    ")\n",
    "\n",
    "tar_num_links, tar_train_data, tar_train_label, tar_val_data, tar_val_label = sliding_window(\n",
    "    tar_near_road, tar_flow, k, t_p, t_input, t_pre, batch_size, train_prop=0.1\n",
    ")\n",
    "\n",
    "src_train_dataset = TensorDataset(src_train_data, src_train_label)\n",
    "tar_train_dataset = TensorDataset(tar_train_data, tar_train_label)\n",
    "\n",
    "src_train_loader = torch.utils.data.DataLoader(src_train_dataset, batch_size, shuffle=True)\n",
    "tar_train_loader = torch.utils.data.DataLoader(tar_train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "#num_links, train_loader, image_val, label_val = sliding_window(near_road, flow, k, t_p, t_input, t_pre, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.88790\n",
      "SMAPE: 0.54527\n",
      "MAE: 14.09851\n"
     ]
    }
   ],
   "source": [
    "traff_dannet = torch.load('../model/net_convnet_final.pth')\n",
    "\n",
    "traff_dannet.eval()\n",
    "\n",
    "val_out, val_out, mmd_loss = traff_dannet(tar_val_data, tar_val_data)\n",
    "#val_out_denormed = denorm_data(val_out, tar_min, tar_max)\n",
    "#tar_val_label_denormed = denorm_data(tar_val_label, tar_min, tar_max)\n",
    "\n",
    "print('MAPE: %.5f'%mape_loss_func(val_out, tar_val_label))\n",
    "print('SMAPE: %.5f'%smape_loss_func(val_out, tar_val_label))\n",
    "print('MAE: %.5f'%mae_loss_func(val_out, tar_val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan_traff",
   "language": "python",
   "name": "dan_traff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
