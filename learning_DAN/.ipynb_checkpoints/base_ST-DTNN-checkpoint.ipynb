{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.keras as keras\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import dan_models\n",
    "import dan_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 class(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10169\\anaconda3\\envs\\dan_traff\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "randseed = 25\n",
    "dan_utils.setup_seed(randseed)\n",
    "res = 11\n",
    "\n",
    "v = pd.read_csv('../data/v_20_aggragated.csv')\n",
    "v = v.rename(columns={'Unnamed: 0': 'id'})\n",
    "det_with_class = pd.read_csv('../res/%i_res%i_id_402_withclass.csv'%(randseed, res), index_col=0)\n",
    "\n",
    "v['class_i'] = ''\n",
    "for i in range(len(v)):\n",
    "    v.loc[i, 'class_i'] = det_with_class[det_with_class['id']==v.loc[i, 'id']].iloc[0, 5]  # 5 stands for 'class_i'\n",
    "\n",
    "num_class = det_with_class['class_i'].drop_duplicates().size\n",
    "\n",
    "v_class = []\n",
    "for i in range(num_class):\n",
    "    v_class.append(v[v['class_i']==i])\n",
    "\n",
    "print('There are %i class(es)'%num_class)\n",
    "\n",
    "dist_mat = pd.read_csv('../data/dist_mat.csv', index_col=0)\n",
    "id_info = pd.read_csv('../data/id2000.csv', index_col=0)\n",
    "dist_mat.index = id_info['id2']\n",
    "dist_mat.columns = id_info['id2']\n",
    "for i in range(len(dist_mat)):\n",
    "    for j in range(len(dist_mat)):\n",
    "        if i==j:\n",
    "            dist_mat.iloc[i, j] = 0\n",
    "\n",
    "near_id = pd.DataFrame(np.argsort(np.array(dist_mat)), index = id_info['id2'], columns = id_info['id2'])\n",
    "\n",
    "seg = pd.read_csv('../data/segement.csv', header=None)\n",
    "num_dets = 25\n",
    "\n",
    "det_list_class = []\n",
    "for i in range(num_class):\n",
    "    det_list_class_temp, v_class_temp = dan_utils.get_class_with_node(seg, v_class[i])\n",
    "    det_list_class.append(det_list_class_temp)\n",
    "    v_class_temp = v_class_temp[v_class_temp['id'].isin(det_list_class_temp[:num_dets])]\n",
    "    v_class[i] = v_class_temp\n",
    "    \n",
    "near_road_set = []\n",
    "for i in range(num_class):\n",
    "    near_road_set.append(dan_utils.rds_mat(dist_mat, det_list_class[i][:num_dets], seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4464)\n",
      "(1161, 5, 12, 25)\n",
      "(898, 5, 12, 25)\n",
      "(263, 5, 12, 25)\n",
      "(898, 25, 3)\n",
      "(263, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "# ind, class\n",
    "# 0  , blue\n",
    "# 1  , green\n",
    "# 2  , yellow  <--\n",
    "# 3  , black   <--\n",
    "# 4  , red     <--\n",
    "class_color_set = ['b', 'g', 'y', 'black', 'r']\n",
    "class_i = 3\n",
    "\n",
    "near_road = np.array(near_road_set[class_i])\n",
    "flow = v_class[class_i].iloc[:, 2:-1]\n",
    "\n",
    "prop = 24/31  # proportion of training data\n",
    "\n",
    "k, t_p, t_input, t_pre, num_links = 5, 10, 12, 3, 25\n",
    "from_day = 1\n",
    "to_day = 31\n",
    "\n",
    "image_train, image_test, day_train, day_test, label_train, label_test\\\n",
    "= dan_utils.sliding_window(\n",
    "    flow, near_road, from_day, to_day, prop, \n",
    "    k, t_p, t_input, t_pre, num_links\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         [(None, 5, 12, 25)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 5, 12, 25)    100         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 5, 12, 25)    5650        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 5, 12, 25)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 12, 25)    100         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 12, 25)    5650        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 12, 25)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1500)         0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1500)         6000        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1500)         0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 150)          225150      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 75)           11325       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 25, 3)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_HA (InputLayer)           [(None, 25, 3)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge__layer (Merge_Layer)      (None, 25, 3)        150         reshape[0][0]                    \n",
      "                                                                 input_HA[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 254,125\n",
      "Trainable params: 251,025\n",
      "Non-trainable params: 3,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = keras.Input(shape=(k,t_input,num_links), name='input_data')\n",
    "input_HA = keras.Input(shape=(num_links, t_pre), name='input_HA')\n",
    "\n",
    "finish_model = dan_models.build_model(input_data, input_HA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "finish_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = image_train\n",
    "X_HA_train = day_train\n",
    "label_train = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 5s 307ms/step - loss: 3096.2844 - val_loss: 2798.3850\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3019.5224 - val_loss: 2554.9436\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2941.1755 - val_loss: 2227.6953\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2776.3560 - val_loss: 1867.8177\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2532.2979 - val_loss: 1540.4291\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2175.2804 - val_loss: 1470.9834\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1738.9518 - val_loss: 1654.0148\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1426.2293 - val_loss: 1470.9597\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1237.0168 - val_loss: 1471.1746\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1136.3622 - val_loss: 1516.5076\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1088.8120 - val_loss: 1451.5685\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1057.6024 - val_loss: 1384.0503\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1038.1405 - val_loss: 1407.6000\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 989.3192 - val_loss: 1470.9537\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 981.8243 - val_loss: 1486.9637\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 955.3417 - val_loss: 1551.8374\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 932.9471 - val_loss: 1599.0573\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 919.8851 - val_loss: 1549.3732\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 899.3203 - val_loss: 1423.7179\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 900.5802 - val_loss: 1308.7677\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 873.2675 - val_loss: 1270.3896\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 856.1388 - val_loss: 1185.3916\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 828.6501 - val_loss: 1118.3855\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 820.4878 - val_loss: 1038.8671\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 798.0252 - val_loss: 951.2354\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 778.5188 - val_loss: 853.7799\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 760.0194 - val_loss: 796.5812\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 740.8466 - val_loss: 775.0930\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 738.8468 - val_loss: 745.1187\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 724.9322 - val_loss: 708.6604\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 712.2303 - val_loss: 687.3229\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 688.3656 - val_loss: 668.1876\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 690.6786 - val_loss: 652.5973\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 669.0614 - val_loss: 650.4694\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 652.8718 - val_loss: 637.7437\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 642.4861 - val_loss: 621.0793\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 628.5758 - val_loss: 606.6568\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 605.5638 - val_loss: 607.1770\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 600.6833 - val_loss: 577.1789\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 585.7923 - val_loss: 594.6868\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 575.5925 - val_loss: 597.4161\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 575.4064 - val_loss: 551.5421\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 564.6675 - val_loss: 533.2987\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 553.5626 - val_loss: 520.6923\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 541.8769 - val_loss: 510.9370\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 529.5972 - val_loss: 525.0370\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 514.6492 - val_loss: 514.5189\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 526.6656 - val_loss: 502.0834\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 501.7511 - val_loss: 468.5250\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 499.3051 - val_loss: 467.0098\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 489.6503 - val_loss: 475.0359\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 476.4404 - val_loss: 494.4784\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 471.7559 - val_loss: 447.5579\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 466.4509 - val_loss: 439.4328\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 461.1968 - val_loss: 427.1373\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 450.7736 - val_loss: 420.3538\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 445.0522 - val_loss: 428.3244\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 431.4898 - val_loss: 434.8138\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 427.2277 - val_loss: 411.8538\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 423.8927 - val_loss: 400.7697\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 419.7309 - val_loss: 400.2146\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 410.8951 - val_loss: 417.0558\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 409.5947 - val_loss: 408.4141\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 389.4023 - val_loss: 384.7195\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 388.9341 - val_loss: 369.0842\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 381.4352 - val_loss: 360.7447\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 378.2554 - val_loss: 376.2922\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 368.4623 - val_loss: 351.5917\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 363.6549 - val_loss: 356.7131\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 356.7182 - val_loss: 354.5479\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 359.9204 - val_loss: 328.0233\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 347.3189 - val_loss: 317.9838\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 343.7873 - val_loss: 313.4506\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 333.0172 - val_loss: 325.0124\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 329.3203 - val_loss: 306.9864\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 319.6354 - val_loss: 299.9618\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 316.7789 - val_loss: 295.3388\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 310.3567 - val_loss: 293.3637\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 308.5263 - val_loss: 280.3284\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 306.1393 - val_loss: 274.8107\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 309.1649 - val_loss: 271.0162\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 294.5230 - val_loss: 269.4679\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 284.6088 - val_loss: 267.2784\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 280.1802 - val_loss: 262.4241\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 281.6650 - val_loss: 255.9054\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 273.1882 - val_loss: 249.1166\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 276.4010 - val_loss: 249.2533\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 263.8241 - val_loss: 251.9217\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 256.7260 - val_loss: 238.4790\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 251.2108 - val_loss: 234.5531\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 250.0731 - val_loss: 232.2918\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 242.8208 - val_loss: 226.3255\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 244.8010 - val_loss: 218.1209\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 236.3845 - val_loss: 216.1819\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 231.4236 - val_loss: 211.2223\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 232.7247 - val_loss: 208.9167\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 233.0480 - val_loss: 205.8031\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 221.7806 - val_loss: 199.7714\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 221.4590 - val_loss: 194.5966\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 218.9323 - val_loss: 198.3736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c43fca1040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型拟合与评估\n",
    "finish_model.fit([X_train,X_HA_train], label_train, epochs=100, batch_size=128,\n",
    "validation_data=([image_test,day_test], label_test))\n",
    "# finish_model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型预测\n",
    "model_pre = finish_model.predict([image_test,day_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape = 0.25527158840778263\n",
      "smape = 0.2526023363547699\n",
      "mae = 10.605989407584948\n"
     ]
    }
   ],
   "source": [
    "#计算各项误差指标\n",
    "\n",
    "mape_mean = dan_utils.mape_loss_func(model_pre, label_test)\n",
    "smape_mean = dan_utils.smape_loss_func(model_pre, label_test)\n",
    "mae_mean = dan_utils.mae_loss_func(model_pre, label_test)\n",
    "\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存\n",
    "finish_model.save_weights('../model/source_%s.h5'%class_color_set[class_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th link\n",
      "0.22997356120509624\n",
      "2th link\n",
      "0.1935355415922355\n",
      "3th link\n",
      "0.11628256403840749\n",
      "4th link\n",
      "0.21029477585455417\n",
      "5th link\n",
      "0.2773035945893709\n",
      "6th link\n",
      "0.23348698552527372\n",
      "7th link\n",
      "0.1892325250766825\n",
      "8th link\n",
      "0.17038652602353632\n",
      "9th link\n",
      "0.18105823836615886\n",
      "10th link\n",
      "0.33118764985647986\n",
      "11th link\n",
      "0.10421944331455532\n",
      "12th link\n",
      "0.24808171562886341\n",
      "13th link\n",
      "0.32739189994105183\n",
      "14th link\n",
      "0.20607212768361394\n",
      "15th link\n",
      "0.2210488032095213\n",
      "16th link\n",
      "0.10487537230031485\n",
      "17th link\n",
      "0.18475625512552563\n",
      "18th link\n",
      "0.2202433417491724\n",
      "19th link\n",
      "0.1853833935788615\n",
      "20th link\n",
      "0.37369617565313734\n",
      "21th link\n",
      "0.22524423708841385\n",
      "22th link\n",
      "0.27253148998603216\n",
      "23th link\n",
      "0.3194893970558492\n",
      "24th link\n",
      "0.9381974300535522\n",
      "25th link\n",
      "0.3178166656983058\n"
     ]
    }
   ],
   "source": [
    "#计算每条路段的误差\n",
    "mape_list = []\n",
    "for i in range(num_links):\n",
    "    a1 = dan_utils.mape_loss_func(model_pre[:,i,:], label_test[:,i,:])\n",
    "    mape_list.append(a1)\n",
    "    print(str(i+1)+'th link')\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mape_pd = pd.Series(mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    0.104219\n",
       "15    0.104875\n",
       "2     0.116283\n",
       "7     0.170387\n",
       "8     0.181058\n",
       "16    0.184756\n",
       "18    0.185383\n",
       "6     0.189233\n",
       "1     0.193536\n",
       "13    0.206072\n",
       "3     0.210295\n",
       "17    0.220243\n",
       "14    0.221049\n",
       "20    0.225244\n",
       "0     0.229974\n",
       "5     0.233487\n",
       "11    0.248082\n",
       "21    0.272531\n",
       "4     0.277304\n",
       "24    0.317817\n",
       "22    0.319489\n",
       "12    0.327392\n",
       "9     0.331188\n",
       "19    0.373696\n",
       "23    0.938197\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_pd.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
  },
  "kernelspec": {
   "display_name": "dan_traff",
   "language": "python",
   "name": "dan_traff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
