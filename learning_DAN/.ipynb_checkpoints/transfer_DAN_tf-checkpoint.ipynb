{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Layer\n",
    "import dan_models\n",
    "import dan_utils\n",
    "from sklearn.manifold import TSNE\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 class(es)\n",
      "(2772, 5, 12, 40)\n",
      "(2772, 5, 12, 40)\n",
      "(0, 5, 12, 40)\n",
      "(2772, 40, 6)\n",
      "(0, 40, 6)\n",
      "(1008, 5, 12, 40)\n",
      "(302, 5, 12, 40)\n",
      "(706, 5, 12, 40)\n",
      "(302, 40, 6)\n",
      "(706, 40, 6)\n"
     ]
    }
   ],
   "source": [
    "class_set = [2, 3, 4]\n",
    "randseed = 25\n",
    "res = 11\n",
    "v, v_class, id_402, part1, part2, seg, det_list_class, near_road_set  \\\n",
    "        = dan_utils.load_data(class_set, res, randseed)\n",
    "class_color_set = ['b', 'g', 'y', 'black', 'r']\n",
    "\n",
    "# ind, class\n",
    "# 0  , blue\n",
    "# 1  , green\n",
    "# 2  , yellow  <--\n",
    "# 3  , black   <--\n",
    "# 4  , red     <--\n",
    "class_src = 3\n",
    "v_class1 = v_class[class_src]  # source\n",
    "near_road1 = np.array(near_road_set[class_src])\n",
    "\n",
    "class_tar = 2\n",
    "v_class2 = v_class[class_tar]  # target\n",
    "near_road2 = np.array(near_road_set[class_tar])\n",
    "\n",
    "num_links = v_class1.shape[0]\n",
    "\n",
    "near_road_src = near_road1\n",
    "flow_src = v_class1.iloc[:, 2:-1]\n",
    "prop = 1  # proportion of training data\n",
    "from_day = 1\n",
    "to_day = 24\n",
    "\n",
    "image_train_source, image_test_source, day_train_source, day_test_source, label_train_source, label_test_source\\\n",
    "= dan_utils.sliding_window(\n",
    "    flow_src, near_road_src, from_day, to_day, prop, num_links\n",
    ")\n",
    "\n",
    "near_road_tar = near_road2\n",
    "flow_tar = v_class2.iloc[:, 2:-1]\n",
    "prop = 3/10\n",
    "from_day = 22\n",
    "to_day = 31\n",
    "\n",
    "image_train_target, image_test_target, day_train_target, day_test_target, label_train_target, label_test_target\\\n",
    "= dan_utils.sliding_window(\n",
    "    flow_tar, near_road_tar, from_day, to_day, prop, num_links\n",
    ")\n",
    "\n",
    "dup_mul = image_train_source.shape[0]//image_train_target.shape[0]\n",
    "dup_r   = image_train_source.shape[0]%image_train_target.shape[0]\n",
    "\n",
    "image_train_target, day_train_target, label_train_target = \\\n",
    "np.concatenate((np.tile(image_train_target, [dup_mul, 1, 1, 1]), image_train_target[:dup_r, :, :, :]), axis=0),\\\n",
    "np.concatenate((np.tile(day_train_target, [dup_mul, 1, 1]), day_train_target[:dup_r, :, :]), axis=0),\\\n",
    "np.concatenate((np.tile(label_train_target, [dup_mul, 1, 1]), label_train_target[:dup_r, :, :]), axis=0),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.InteractiveSession()\n",
    "from numpy import shape\n",
    "\n",
    "def Weight(Shape):\n",
    "    return tf.Variable(tf.compat.v1.truncated_normal(Shape, stddev=0.1))\n",
    "\n",
    "\n",
    "def Bias(Shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=Shape))\n",
    "\n",
    "\n",
    "def Conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def Pool(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1,2,2,1], strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "x = tf.compat.v1.placeholder(\"float\", [None, 5, 12, 30])  # 4维 [batch_size, height, width, channels]\n",
    "xt = tf.compat.v1.placeholder(\"float\", [None, 5, 12, 30])\n",
    "t = tf.compat.v1.placeholder(\"float\", [None, 30, 3])  # 真值\n",
    "# external = tf.placeholder(\"float\", [None,7])  # 7：前6代表time of day 后1代表 day of week    1-n排序\n",
    "# week = tf.placeholder(\"float\", [None,1196,6])  # 历史信息\n",
    "\n",
    "# change the external variable\n",
    "# W0 = Weight([7,1196*6])\n",
    "# External = tf.reshape(tf.matmul(external, W0),[-1,1196,6])\n",
    "\n",
    "# two convolutional layers\n",
    "W1 = Weight([3,3,30,30])  # [filter_height, filter_width, in_channels, out_channels]\n",
    "b1 = Bias([30])\n",
    "conv1 = tf.nn.relu(Conv2d(tf.compat.v1.layers.batch_normalization(x), W1) + b1)\n",
    "pool1 = Pool(conv1)  # conv1: [batch, height, width, channels]\n",
    "conv1t = tf.nn.relu(Conv2d(tf.compat.v1.layers.batch_normalization(xt), W1) + b1)\n",
    "pool1t = Pool(conv1t)\n",
    "# 第2层\n",
    "W2 = Weight([3,3,30,30])\n",
    "b2 = Bias([30])\n",
    "conv2 = tf.nn.relu(Conv2d(tf.compat.v1.layers.batch_normalization(pool1), W2) + b2)\n",
    "pool2 = Pool(conv2)  # 输出[4536, 12, 12, 30]   因为'SAME'\n",
    "\n",
    "conv2t = tf.nn.relu(Conv2d(tf.compat.v1.layers.batch_normalization(pool1t), W2) + b2)\n",
    "pool2t = Pool(conv2t)  # 输出[4536, 12, 12, 30]   因为'SAME'\n",
    "# two fully-connected layer\n",
    "input_Connect = tf.reshape(pool2, [-1,5*12*30])\n",
    "input_Connect_t = tf.reshape(pool2t, [-1,5*12*30])\n",
    "\n",
    "W3 = Weight([5*12*30,5*12*30])\n",
    "b3 = Bias([5*12*30])\n",
    "fc = tf.nn.relu(tf.matmul(input_Connect, W3) + b3)  # [-1, 30*6]\n",
    "fc_t = tf.nn.relu(tf.matmul(input_Connect_t, W3) + b3)  # [-1, 30*6]\n",
    "\n",
    "# W3 = Weight([5*12*30,5*12*30])\n",
    "# b3 = Bias([5*12*30])\n",
    "# fc = tf.nn.relu(tf.matmul(tf.layers.batch_normalization(input_Connect), W3) + b3)  # [-1, 30*6]\n",
    "# fc_t = tf.nn.relu(tf.matmul(tf.layers.batch_normalization(input_Connect_t), W3) + b3)  # [-1, 30*6]\n",
    "\n",
    "\n",
    "# W4 = Weight([5*12*30,30*3])\n",
    "# b4 = Bias([30*3])\n",
    "# output_Connect = tf.nn.relu(tf.matmul(tf.layers.batch_normalization(fc), W4) + b4)  # [-1, 30*6]\n",
    "# output_Connect_t = tf.nn.relu(tf.matmul(tf.layers.batch_normalization(fc_t), W4) + b4)  # [-1, 30*6]\n",
    "\n",
    "W4 = Weight([5*12*30,30*3])\n",
    "b4 = Bias([30*3])\n",
    "output_Connect = tf.nn.relu(tf.matmul(fc, W4) + b4)  # [-1, 30*6]\n",
    "output_Connect_t = tf.nn.relu(tf.matmul(fc_t, W4) + b4)  # [-1, 30*6]\n",
    "\n",
    "Output_cnn = tf.reshape(output_Connect, [-1,30,3])\n",
    "\n",
    "y = tf.nn.relu(Output_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#已验证两种核函数结果同wjd代码\n",
    "def rbf_kernel(x, y, gamma=0.001):\n",
    "    x_3d_i = tf.expand_dims(x, 1)\n",
    "    y_3d_j = tf.expand_dims(y, 0)\n",
    "    kernel = tf.reduce_sum((x_3d_i - y_3d_j) ** 2, 2)\n",
    "    kernel = tf.exp(- gamma * kernel)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def mmd_rbf(X, Y, gamma=1000):\n",
    "    XX = rbf_kernel(X, X, gamma)\n",
    "    YY = rbf_kernel(Y, Y, gamma)\n",
    "    XY = rbf_kernel(X, Y, gamma)\n",
    "    # return XX + YY - 2*XY\n",
    "    return tf.reduce_mean(XX) + tf.reduce_mean(YY) -2*tf.reduce_mean(XY)\n",
    "\n",
    "\n",
    "def mmd_linear_tf(X,Y):\n",
    "    XX = tf.matmul(X, tf.transpose(X))\n",
    "    YY = tf.matmul(Y, tf.transpose(Y))\n",
    "    XY = tf.matmul(X, tf.transpose(Y))\n",
    "    return tf.reduce_mean(XX) + tf.reduce_mean(YY) -2*tf.reduce_mean(XY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Variable_4:0', 'Variable_6:0', 'Variable_8:0', 'Variable_10:0', 'Variable_5:0', 'Variable_7:0', 'Variable_9:0', 'Variable_11:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19060/229877981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnewloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnrmse\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdomain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dan_traff\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[0;32m    496\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m    497\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[1;32m--> 498\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_compute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dan_traff\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \"\"\"\n\u001b[1;32m--> 598\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dan_traff\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[0;32m     79\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable_4:0', 'Variable_6:0', 'Variable_8:0', 'Variable_10:0', 'Variable_5:0', 'Variable_7:0', 'Variable_9:0', 'Variable_11:0']."
     ]
    }
   ],
   "source": [
    "\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.subtract(t,y)**2))\n",
    "nrmse = tf.sqrt(tf.reduce_sum(tf.subtract(t,y)**2)/tf.reduce_sum(tf.square(t)))\n",
    "mape = tf.reduce_mean((abs(tf.subtract(t,y)))/(abs(t)))\n",
    "Rsqure = 1-(tf.reduce_sum(tf.subtract(t,y)**2))/(tf.reduce_sum((t-tf.reduce_mean(t))**2))\n",
    "\n",
    "# domain_loss = tf.reduce_sum(mmd_rbf(output_Connect, output_Connect_t, gamma = 1))\n",
    "domain_loss = mmd_rbf(fc, fc_t)\n",
    "\n",
    "newloss = nrmse + 0.1*domain_loss\n",
    "\n",
    "train_step = tf.optimizers.Adam(0.001).minimize(newloss, [W1,W2,W3,W4,b1,b2,b3,b4],tape=tf.GradientTape())\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch ---------------10\n",
      "training time 0.171632\n",
      "test rmse 18.8491\n",
      "test nrmse 0.932196\n",
      "test mape 0.886697\n",
      "test domain loss 0.004\n",
      "train epoch ---------------20\n",
      "training time 0.170963\n",
      "test rmse 18.8422\n",
      "test nrmse 0.931851\n",
      "test mape 0.88511\n",
      "test domain loss 0.004\n",
      "train epoch ---------------30\n",
      "training time 0.171709\n",
      "test rmse 18.6981\n",
      "test nrmse 0.924725\n",
      "test mape 0.875692\n",
      "test domain loss 0.004\n",
      "train epoch ---------------40\n",
      "training time 0.167102\n",
      "test rmse 18.6945\n",
      "test nrmse 0.924549\n",
      "test mape 0.87407\n",
      "test domain loss 0.004\n",
      "train epoch ---------------50\n",
      "training time 0.171786\n",
      "test rmse 16.9761\n",
      "test nrmse 0.839563\n",
      "test mape 0.73042\n",
      "test domain loss 0.004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a6d8e5d0db1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mb_idx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_train_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mimage_train_source\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mimage_train_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabel_train_source\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mb_idx\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mb_size\u001b[0m \u001b[1;31m#更新batch计数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T=300 #训练几epoch\n",
    "b_size=50 #batch大小\n",
    "\n",
    "for i in range(1,T+1):\n",
    "    b_idx=0 #batch计数\n",
    "    time1 = time.time()\n",
    "    while b_idx<shape(image_train_target)[0]-b_size:\n",
    "        sess.run(train_step,feed_dict={x:image_train_source[b_idx:b_idx+b_size], xt:image_train_target[b_idx:b_idx+b_size],t:label_train_source[b_idx:b_idx+b_size]})\n",
    "\n",
    "        b_idx+=b_size #更新batch计数\n",
    "    if i%10==0:\n",
    "        time2 = time.time()\n",
    "        print(\"train epoch ---------------%g\"%i)\n",
    "        print(\"training time %g\"%(time2-time1))\n",
    "        print(\"test rmse %g\"%rmse.eval(feed_dict={x:image_test_source, t:label_test_source}))\n",
    "        print(\"test nrmse %g\"%nrmse.eval(feed_dict={x:image_test_source, t:label_test_source}))\n",
    "        print(\"test mape %g\"%mape.eval(feed_dict={x:image_test_source, t:label_test_source}))\n",
    "        print(\"test domain loss %g\"%domain_loss.eval(feed_dict={x:image_train_source[:500], xt:image_train_source[:500]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 5, 12, 30)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 12, 30)    120         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 12, 30)    8130        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 12, 30)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 12, 30)    120         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 12, 30)    8130        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 5, 12, 30)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1800)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1800)         7200        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1800)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 90)           162090      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 30, 3)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_HA (InputLayer)           (None, 30, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge__layer_1 (Merge_Layer)    (None, 30, 3)        180         reshape_1[0][0]                  \n",
      "                                                                 input_HA[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 185,970\n",
      "Trainable params: 182,250\n",
      "Non-trainable params: 3,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = keras.Input(shape=(k,t_input,num_links), name='input_data')\n",
    "input_HA = keras.Input(shape=(num_links, t_pre), name='input_HA')\n",
    "\n",
    "x = keras.layers.BatchNormalization(input_shape =(k,t_input,num_links))(input_data)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                           filters = 30,\n",
    "                           kernel_size = 3,\n",
    "                           strides = 1,\n",
    "                           padding=\"SAME\",\n",
    "                           activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                       filters = 30,\n",
    "                       kernel_size = 3,\n",
    "                       strides = 1,\n",
    "                       padding=\"SAME\",\n",
    "                       activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(num_links*t_pre, activation='relu')(x)\n",
    "\n",
    "output = keras.layers.Reshape((num_links,t_pre))(x)\n",
    "\n",
    "output_final = Merge_Layer()([output, input_HA])\n",
    "\n",
    "# construct model\n",
    "finish_model = keras.models.Model([input_data,input_HA], [output_final])\n",
    "\n",
    "finish_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finish_model.compile(optimizer='adam',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = image_train\n",
    "X_HA_train = day_train\n",
    "label_train = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3696 samples, validate on 924 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: cuDNN launch failure : input shape ([128,30,5,12])\n\t [[{{node batch_normalization_1/FusedBatchNorm}}]]\n\t [[loss/mul/_219]]\n  (1) Internal: cuDNN launch failure : input shape ([128,30,5,12])\n\t [[{{node batch_normalization_1/FusedBatchNorm}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fa2d2fdd72c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#模型拟合与评估\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m finish_model.fit([X_train,X_HA_train], label_train, epochs=400, batch_size=128,\n\u001b[1;32m----> 3\u001b[1;33m validation_data=([image_test,day_test], label_test))\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# finish_model.evaluate(image_test, label_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: cuDNN launch failure : input shape ([128,30,5,12])\n\t [[{{node batch_normalization_1/FusedBatchNorm}}]]\n\t [[loss/mul/_219]]\n  (1) Internal: cuDNN launch failure : input shape ([128,30,5,12])\n\t [[{{node batch_normalization_1/FusedBatchNorm}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "#模型拟合与评估\n",
    "finish_model.fit([X_train,X_HA_train], label_train, epochs=400, batch_size=128,\n",
    "validation_data=([image_test,day_test], label_test))\n",
    "# finish_model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型预测\n",
    "model_pre = finish_model.predict([image_test,day_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape = 0.10452546131084063\n",
      "smape = 0.104873171322513\n",
      "mae = 2.0524827601591373\n"
     ]
    }
   ],
   "source": [
    "#计算各项误差指标\n",
    "\n",
    "mape_mean = mape_loss_func(model_pre, label_test)\n",
    "smape_mean = smape_loss_func(model_pre, label_test)\n",
    "mae_mean = mae_loss_func(model_pre, label_test)\n",
    "\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存\n",
    "# finish_model.save_weights('参数-融合-速度.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th link\n",
      "0.0943957348515972\n",
      "2th link\n",
      "0.09029504620075238\n",
      "3th link\n",
      "0.08105435884304042\n",
      "4th link\n",
      "0.0932327012510816\n",
      "5th link\n",
      "0.1029118834088521\n",
      "6th link\n",
      "0.08965855683987796\n",
      "7th link\n",
      "0.11854642862675781\n",
      "8th link\n",
      "0.096406513998008\n",
      "9th link\n",
      "0.09967157153635753\n",
      "10th link\n",
      "0.10556153858293499\n",
      "11th link\n",
      "0.09991127307547765\n",
      "12th link\n",
      "0.117393015681981\n",
      "13th link\n",
      "0.10959950434341895\n",
      "14th link\n",
      "0.09962537482918044\n",
      "15th link\n",
      "0.12371702544347449\n",
      "16th link\n",
      "0.10117730585134602\n",
      "17th link\n",
      "0.10911830689423327\n",
      "18th link\n",
      "0.09335764334138849\n",
      "19th link\n",
      "0.10489256070494483\n",
      "20th link\n",
      "0.13568377602249643\n",
      "21th link\n",
      "0.1171051201213036\n",
      "22th link\n",
      "0.09863602179006947\n",
      "23th link\n",
      "0.08754855370472213\n",
      "24th link\n",
      "0.09091265930851955\n",
      "25th link\n",
      "0.11026106447061013\n",
      "26th link\n",
      "0.09380021841562518\n",
      "27th link\n",
      "0.10873440790166558\n",
      "28th link\n",
      "0.11966390814857954\n",
      "29th link\n",
      "0.12183276561694073\n",
      "30th link\n",
      "0.12105899951998168\n"
     ]
    }
   ],
   "source": [
    "#计算每条路段的误差\n",
    "mape_list = []\n",
    "for i in range(num_links):\n",
    "    a1 = mape_loss_func(model_pre[:,i,:], label_test[:,i,:])\n",
    "    mape_list.append(a1)\n",
    "    print(str(i+1)+'th link')\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mape_pd = pd.Series(mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     0.081054\n",
       "22    0.087549\n",
       "5     0.089659\n",
       "1     0.090295\n",
       "23    0.090913\n",
       "3     0.093233\n",
       "17    0.093358\n",
       "25    0.093800\n",
       "0     0.094396\n",
       "7     0.096407\n",
       "21    0.098636\n",
       "13    0.099625\n",
       "8     0.099672\n",
       "10    0.099911\n",
       "15    0.101177\n",
       "4     0.102912\n",
       "18    0.104893\n",
       "9     0.105562\n",
       "26    0.108734\n",
       "16    0.109118\n",
       "12    0.109600\n",
       "24    0.110261\n",
       "20    0.117105\n",
       "11    0.117393\n",
       "6     0.118546\n",
       "27    0.119664\n",
       "29    0.121059\n",
       "28    0.121833\n",
       "14    0.123717\n",
       "19    0.135684\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_pd.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
  },
  "kernelspec": {
   "display_name": "dan_traff",
   "language": "python",
   "name": "dan_traff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
