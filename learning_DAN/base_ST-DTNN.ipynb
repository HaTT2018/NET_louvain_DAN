{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss_func(preds, labels):\n",
    "    mask = labels > 5\n",
    "    return np.mean(np.fabs(labels[mask]-preds[mask])/labels[mask])\n",
    "\n",
    "def smape_loss_func(preds, labels):\n",
    "    mask= labels > 5\n",
    "    return np.mean(2*np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\n",
    "\n",
    "def mae_loss_func(preds, labels):\n",
    "    mask= labels > 5\n",
    "    return np.fabs((labels[mask]-preds[mask])).mean()\n",
    "\n",
    "def eliminate_nan(b):\n",
    "    a = np.array(b)\n",
    "    c = a[~np.isnan(a)]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制作flow 和 near road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "randseed = 25\n",
    "res = 11\n",
    "num_dets = 25\n",
    "\n",
    "v = pd.read_csv('../data/v_20_aggragated.csv')\n",
    "v = v.rename(columns={'Unnamed: 0': 'id'})\n",
    "# 以上 flow 制作完毕\n",
    "\n",
    "dist_mat = pd.read_csv('../data/dist_mat.csv', index_col=0)\n",
    "id_info = pd.read_csv('../data/id2000.csv', index_col=0)\n",
    "dist_mat.index = id_info['id2']\n",
    "dist_mat.columns = id_info['id2']\n",
    "for i in range(len(dist_mat)):\n",
    "    for j in range(len(dist_mat)):\n",
    "        if i==j:\n",
    "            dist_mat.iloc[i, j] = 0\n",
    "\n",
    "near_id = pd.DataFrame(np.argsort(np.array(dist_mat.iloc[:num_dets, :num_dets])), index = id_info['id2'][:num_dets], columns = id_info['id2'][:num_dets])\n",
    "# 以上near_road 制作完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入最近路段数据，流量数据\n",
    "# distance = np.array(pd.read_csv('video_link_dis.csv',header = None))\n",
    "near_road = np.array(near_id)\n",
    "flow = np.array(v.iloc[:, 1:]) #注意header=0 or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 5, 12, 25)\n",
      "(1290, 25, 3)\n",
      "(1290, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "# 利用滑动窗口的方式，重构数据为(n，最近路段数，输入时间窗，总路段数)的形式\n",
    "k = 5 # 参数k为需考虑的最近路段数\n",
    "t_p = 31 # 参数t_p为总时间序列长度（天）\n",
    "t_input = 12 #参数t_input为输入时间窗(10min颗粒度)\n",
    "t_pre = 3 #参数t_pre为预测时间窗(10min颗粒度) 颗粒度？\n",
    "num_links = num_dets #参数num_links为总路段数\n",
    "flow = flow[:num_links, :]\n",
    "g = 10\n",
    "n = 24*60//g\n",
    "\n",
    "\n",
    "image = []\n",
    "for i in range(np.shape(near_road)[0]):\n",
    "    road_id = []\n",
    "    for j in range(k):\n",
    "        road_id.append(near_road[i][j])\n",
    "    image.append(flow[road_id, :])\n",
    "image1 = np.reshape(image, [-1, k, len(flow[0,:])])\n",
    "image2 = np.transpose(image1,(1,2,0))\n",
    "image3 = []\n",
    "label = []\n",
    "day = []\n",
    "\n",
    "for i in range(21, t_p):  # 10 days in total\n",
    "    for j in range(n-t_input-t_pre):\n",
    "        image3.append(image2[:, i*n+j:i*n+j+t_input, :][:])\n",
    "        label.append(flow[:, i*n+j+t_input:i*n+j+t_input+t_pre][:])\n",
    "        day.append(flow[:, (i-1)*n+j+t_input:(i-1)*n+j+t_input+t_pre][:])\n",
    "\n",
    "image3 = np.asarray(image3)\n",
    "label = np.asarray(label)\n",
    "day =  np.asarray(day)\n",
    "\n",
    "print(np.shape(image3))\n",
    "print(np.shape(label))\n",
    "print(np.shape(day))\n",
    "\n",
    "#划分前80%数据为训练集，最后20%数据为测试集\n",
    "prop = 0.3\n",
    "image_train = image3[:int(np.shape(image3)[0]*prop)]\n",
    "image_test = image3[int(np.shape(image3)[0]*prop):]\n",
    "label_train = label[:int(np.shape(label)[0]*prop)]\n",
    "label_test = label[int(np.shape(label)[0]*prop):]\n",
    "\n",
    "day_train = day[:int(np.shape(day)[0]*prop)]\n",
    "day_test = day[int(np.shape(day)[0]*prop):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define merge layer\n",
    "class Merge_Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Merge_Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.para1 = self.add_weight(shape=(input_shape[0][1], input_shape[0][2]),\n",
    "                                     initializer='uniform', trainable=True,\n",
    "                                     name='para1')\n",
    "        self.para2 = self.add_weight(shape=(input_shape[1][1], input_shape[1][2]),\n",
    "                                     initializer='uniform', trainable=True,\n",
    "                                     name='para2')\n",
    "        super(Merge_Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mat1 = inputs[0]\n",
    "        mat2 = inputs[1]\n",
    "        output = mat1 * self.para1 + mat2 * self.para2\n",
    "        # output = mat1 * 0.1 + mat2 * 0.9\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 5, 12, 25)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 12, 25)    100         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 12, 25)    5650        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 12, 25)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 12, 25)    100         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 12, 25)    5650        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 5, 12, 25)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1500)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1500)         6000        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1500)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 150)          225150      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 75)           11325       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 25, 3)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_HA (InputLayer)           (None, 25, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge__layer_1 (Merge_Layer)    (None, 25, 3)        150         reshape_1[0][0]                  \n",
      "                                                                 input_HA[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 254,125\n",
      "Trainable params: 251,025\n",
      "Non-trainable params: 3,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = keras.Input(shape=(k,t_input,num_links), name='input_data')\n",
    "input_HA = keras.Input(shape=(num_links, t_pre), name='input_HA')\n",
    "\n",
    "x = keras.layers.BatchNormalization(input_shape =(k,t_input,num_links))(input_data)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                           filters = num_links,\n",
    "                           kernel_size = 3,\n",
    "                           strides = 1,\n",
    "                           padding=\"SAME\",\n",
    "                           activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                       filters = num_links,\n",
    "                       kernel_size = 3,\n",
    "                       strides = 1,\n",
    "                       padding=\"SAME\",\n",
    "                       activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = keras.layers.Dense(num_links*2*t_pre, activation='relu', name='dense_1')(x)\n",
    "x = keras.layers.Dense(num_links*t_pre, activation='relu', name='dense_2')(x)\n",
    "\n",
    "output = keras.layers.Reshape((num_links,t_pre))(x)\n",
    "\n",
    "output_final = Merge_Layer()([output, input_HA])\n",
    "\n",
    "# construct model\n",
    "finish_model = keras.models.Model([input_data,input_HA], [output_final])\n",
    "\n",
    "finish_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finish_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = image_train\n",
    "X_HA_train = day_train\n",
    "label_train = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 387 samples, validate on 903 samples\n",
      "Epoch 1/350\n",
      "387/387 [==============================] - 1s 3ms/step - loss: 3807.7144 - val_loss: 3873.3929\n",
      "Epoch 2/350\n",
      "387/387 [==============================] - 0s 168us/step - loss: 3774.3647 - val_loss: 3835.1199\n",
      "Epoch 3/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 3737.5157 - val_loss: 3786.9908\n",
      "Epoch 4/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 3695.2071 - val_loss: 3730.4072\n",
      "Epoch 5/350\n",
      "387/387 [==============================] - 0s 160us/step - loss: 3644.4502 - val_loss: 3657.3867\n",
      "Epoch 6/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 3580.3323 - val_loss: 3565.8033\n",
      "Epoch 7/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 3495.2680 - val_loss: 3447.0082\n",
      "Epoch 8/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 3386.5385 - val_loss: 3290.4330\n",
      "Epoch 9/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 3247.1474 - val_loss: 3114.9903\n",
      "Epoch 10/350\n",
      "387/387 [==============================] - 0s 168us/step - loss: 3068.5674 - val_loss: 2896.1583\n",
      "Epoch 11/350\n",
      "387/387 [==============================] - 0s 148us/step - loss: 2852.7825 - val_loss: 2626.3759\n",
      "Epoch 12/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 2598.9724 - val_loss: 2317.8247\n",
      "Epoch 13/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 2336.8120 - val_loss: 2020.4720\n",
      "Epoch 14/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 2067.2596 - val_loss: 1819.4595\n",
      "Epoch 15/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1843.6280 - val_loss: 1774.8972\n",
      "Epoch 16/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 1700.5208 - val_loss: 1747.7332\n",
      "Epoch 17/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 1573.3090 - val_loss: 1709.7899\n",
      "Epoch 18/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 1540.6081 - val_loss: 1633.8662\n",
      "Epoch 19/350\n",
      "387/387 [==============================] - 0s 160us/step - loss: 1459.2896 - val_loss: 1566.8873\n",
      "Epoch 20/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1423.0687 - val_loss: 1518.0407\n",
      "Epoch 21/350\n",
      "387/387 [==============================] - 0s 150us/step - loss: 1395.8039 - val_loss: 1491.7587\n",
      "Epoch 22/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1392.0575 - val_loss: 1457.4665\n",
      "Epoch 23/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1367.5819 - val_loss: 1445.3635\n",
      "Epoch 24/350\n",
      "387/387 [==============================] - 0s 157us/step - loss: 1330.4145 - val_loss: 1440.0994\n",
      "Epoch 25/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 1317.3547 - val_loss: 1420.8520\n",
      "Epoch 26/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1293.4483 - val_loss: 1408.8916\n",
      "Epoch 27/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 1273.7861 - val_loss: 1396.2303\n",
      "Epoch 28/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 1266.2832 - val_loss: 1386.5932\n",
      "Epoch 29/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1232.2998 - val_loss: 1396.8652\n",
      "Epoch 30/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1223.0798 - val_loss: 1385.3980\n",
      "Epoch 31/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 1212.4200 - val_loss: 1351.6022\n",
      "Epoch 32/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 1215.3194 - val_loss: 1331.8606\n",
      "Epoch 33/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 1194.3692 - val_loss: 1322.4773\n",
      "Epoch 34/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1196.9070 - val_loss: 1326.0325\n",
      "Epoch 35/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 1173.6448 - val_loss: 1307.6941\n",
      "Epoch 36/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1166.6455 - val_loss: 1289.1205\n",
      "Epoch 37/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 1142.8849 - val_loss: 1264.0852\n",
      "Epoch 38/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1130.6729 - val_loss: 1246.8821\n",
      "Epoch 39/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 1128.8009 - val_loss: 1224.9894\n",
      "Epoch 40/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 1167.2308 - val_loss: 1215.0878\n",
      "Epoch 41/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 1119.9842 - val_loss: 1221.4690\n",
      "Epoch 42/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1114.8003 - val_loss: 1251.0024\n",
      "Epoch 43/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 1110.7593 - val_loss: 1238.8388\n",
      "Epoch 44/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 1076.8968 - val_loss: 1184.3320\n",
      "Epoch 45/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 1073.9912 - val_loss: 1143.4855\n",
      "Epoch 46/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 1061.5467 - val_loss: 1127.3748\n",
      "Epoch 47/350\n",
      "387/387 [==============================] - 0s 165us/step - loss: 1056.8300 - val_loss: 1119.7874\n",
      "Epoch 48/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 1032.2345 - val_loss: 1113.5856\n",
      "Epoch 49/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 1021.5852 - val_loss: 1107.5094\n",
      "Epoch 50/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1025.5758 - val_loss: 1100.2511\n",
      "Epoch 51/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 1006.2747 - val_loss: 1088.2884\n",
      "Epoch 52/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 991.8391 - val_loss: 1073.6083\n",
      "Epoch 53/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 990.1819 - val_loss: 1061.2835\n",
      "Epoch 54/350\n",
      "387/387 [==============================] - 0s 158us/step - loss: 982.6893 - val_loss: 1051.7401\n",
      "Epoch 55/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 966.1043 - val_loss: 1047.9048\n",
      "Epoch 56/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 972.4082 - val_loss: 1044.7625\n",
      "Epoch 57/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 954.7548 - val_loss: 1031.8766\n",
      "Epoch 58/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 957.9481 - val_loss: 1025.1004\n",
      "Epoch 59/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 944.8765 - val_loss: 1019.6347\n",
      "Epoch 60/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 934.7604 - val_loss: 1016.1149\n",
      "Epoch 61/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 926.2530 - val_loss: 1003.4477\n",
      "Epoch 62/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 913.1970 - val_loss: 994.7977\n",
      "Epoch 63/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 897.2704 - val_loss: 984.3143\n",
      "Epoch 64/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 892.5596 - val_loss: 973.0948\n",
      "Epoch 65/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 879.3390 - val_loss: 957.8222\n",
      "Epoch 66/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 880.6647 - val_loss: 949.6160\n",
      "Epoch 67/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 867.2387 - val_loss: 954.6467\n",
      "Epoch 68/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 859.4300 - val_loss: 951.5098\n",
      "Epoch 69/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 854.0653 - val_loss: 933.9543\n",
      "Epoch 70/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 843.3015 - val_loss: 914.8264\n",
      "Epoch 71/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 849.7340 - val_loss: 900.2993\n",
      "Epoch 72/350\n",
      "387/387 [==============================] - 0s 160us/step - loss: 834.7340 - val_loss: 891.6651\n",
      "Epoch 73/350\n",
      "387/387 [==============================] - ETA: 0s - loss: 816.307 - 0s 173us/step - loss: 815.9501 - val_loss: 893.4242\n",
      "Epoch 74/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 802.7160 - val_loss: 891.2021\n",
      "Epoch 75/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 803.7001 - val_loss: 888.6221\n",
      "Epoch 76/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 783.9704 - val_loss: 876.7279\n",
      "Epoch 77/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 777.9084 - val_loss: 861.9918\n",
      "Epoch 78/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 779.5419 - val_loss: 846.4521\n",
      "Epoch 79/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 766.5001 - val_loss: 837.2454\n",
      "Epoch 80/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 799.4919 - val_loss: 828.1938\n",
      "Epoch 81/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 757.9259 - val_loss: 825.5020\n",
      "Epoch 82/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 739.9101 - val_loss: 831.6402\n",
      "Epoch 83/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 736.6707 - val_loss: 826.7803\n",
      "Epoch 84/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 738.4726 - val_loss: 829.1827\n",
      "Epoch 85/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 725.0483 - val_loss: 835.9337\n",
      "Epoch 86/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 719.6058 - val_loss: 823.9531\n",
      "Epoch 87/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 721.9307 - val_loss: 798.7810\n",
      "Epoch 88/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 697.2571 - val_loss: 776.7345\n",
      "Epoch 89/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 688.5050 - val_loss: 755.7177\n",
      "Epoch 90/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 689.5284 - val_loss: 742.6426\n",
      "Epoch 91/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 674.5943 - val_loss: 731.8736\n",
      "Epoch 92/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 674.3046 - val_loss: 724.9395\n",
      "Epoch 93/350\n",
      "387/387 [==============================] - 0s 138us/step - loss: 661.8697 - val_loss: 720.4656\n",
      "Epoch 94/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 658.8489 - val_loss: 721.0934\n",
      "Epoch 95/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 648.0788 - val_loss: 731.9402\n",
      "Epoch 96/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 644.6681 - val_loss: 738.0420\n",
      "Epoch 97/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 645.8194 - val_loss: 754.8179\n",
      "Epoch 98/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 653.3208 - val_loss: 756.8522\n",
      "Epoch 99/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 652.5290 - val_loss: 727.0544\n",
      "Epoch 100/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 629.7347 - val_loss: 710.6985\n",
      "Epoch 101/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 617.3323 - val_loss: 684.7346\n",
      "Epoch 102/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 605.2279 - val_loss: 659.2185\n",
      "Epoch 103/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 598.9917 - val_loss: 646.6494\n",
      "Epoch 104/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 590.0133 - val_loss: 644.1619\n",
      "Epoch 105/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 587.5350 - val_loss: 638.7240\n",
      "Epoch 106/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 585.8430 - val_loss: 639.3092\n",
      "Epoch 107/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 588.2447 - val_loss: 642.7000\n",
      "Epoch 108/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 588.9748 - val_loss: 651.3845\n",
      "Epoch 109/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 589.3257 - val_loss: 655.8090\n",
      "Epoch 110/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 579.8693 - val_loss: 641.0694\n",
      "Epoch 111/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 575.5629 - val_loss: 624.9230\n",
      "Epoch 112/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 570.2079 - val_loss: 612.7319\n",
      "Epoch 113/350\n",
      "387/387 [==============================] - 0s 160us/step - loss: 549.7702 - val_loss: 601.2697\n",
      "Epoch 114/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 546.6253 - val_loss: 587.5578\n",
      "Epoch 115/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 539.5106 - val_loss: 572.4803\n",
      "Epoch 116/350\n",
      "387/387 [==============================] - 0s 157us/step - loss: 536.0048 - val_loss: 566.0983\n",
      "Epoch 117/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 530.6936 - val_loss: 561.4505\n",
      "Epoch 118/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 525.4606 - val_loss: 557.3935\n",
      "Epoch 119/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 534.0034 - val_loss: 555.8940\n",
      "Epoch 120/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 517.9212 - val_loss: 560.2512\n",
      "Epoch 121/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 518.5419 - val_loss: 556.6334\n",
      "Epoch 122/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 520.2737 - val_loss: 546.5941\n",
      "Epoch 123/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 504.4600 - val_loss: 536.7487\n",
      "Epoch 124/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 506.0889 - val_loss: 526.2328\n",
      "Epoch 125/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 493.7688 - val_loss: 519.7004\n",
      "Epoch 126/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 493.6391 - val_loss: 514.4824\n",
      "Epoch 127/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 485.1348 - val_loss: 511.3259\n",
      "Epoch 128/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 477.7929 - val_loss: 508.2491\n",
      "Epoch 129/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 476.0547 - val_loss: 508.5007\n",
      "Epoch 130/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 472.1635 - val_loss: 505.6126\n",
      "Epoch 131/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 468.6857 - val_loss: 500.8249\n",
      "Epoch 132/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 462.1807 - val_loss: 493.0060\n",
      "Epoch 133/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 464.9954 - val_loss: 486.4935\n",
      "Epoch 134/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 458.3815 - val_loss: 481.4171\n",
      "Epoch 135/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 450.3341 - val_loss: 483.0275\n",
      "Epoch 136/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 450.6921 - val_loss: 490.5958\n",
      "Epoch 137/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 455.7305 - val_loss: 484.2692\n",
      "Epoch 138/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 445.1177 - val_loss: 476.0658\n",
      "Epoch 139/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 447.0671 - val_loss: 475.3647\n",
      "Epoch 140/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 442.1960 - val_loss: 468.3837\n",
      "Epoch 141/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 434.5511 - val_loss: 458.2656\n",
      "Epoch 142/350\n",
      "387/387 [==============================] - 0s 157us/step - loss: 424.7776 - val_loss: 450.8557\n",
      "Epoch 143/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 421.9981 - val_loss: 446.3998\n",
      "Epoch 144/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 421.7266 - val_loss: 442.8294\n",
      "Epoch 145/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 418.1734 - val_loss: 439.6342\n",
      "Epoch 146/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 414.6866 - val_loss: 439.0476\n",
      "Epoch 147/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 412.3850 - val_loss: 434.8373\n",
      "Epoch 148/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 413.4508 - val_loss: 429.1339\n",
      "Epoch 149/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 401.4226 - val_loss: 422.6661\n",
      "Epoch 150/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 0s 147us/step - loss: 389.2348 - val_loss: 415.3664\n",
      "Epoch 151/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 389.7685 - val_loss: 407.4718\n",
      "Epoch 152/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 384.8967 - val_loss: 402.4170\n",
      "Epoch 153/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 377.3527 - val_loss: 403.4857\n",
      "Epoch 154/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 377.4584 - val_loss: 407.2747\n",
      "Epoch 155/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 371.4055 - val_loss: 407.1596\n",
      "Epoch 156/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 368.9772 - val_loss: 402.8031\n",
      "Epoch 157/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 365.1130 - val_loss: 395.4486\n",
      "Epoch 158/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 360.0493 - val_loss: 388.2679\n",
      "Epoch 159/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 355.8999 - val_loss: 381.9148\n",
      "Epoch 160/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 351.5993 - val_loss: 378.3327\n",
      "Epoch 161/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 350.2107 - val_loss: 375.6470\n",
      "Epoch 162/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 343.0646 - val_loss: 372.0832\n",
      "Epoch 163/350\n",
      "387/387 [==============================] - 0s 150us/step - loss: 339.9904 - val_loss: 369.2392\n",
      "Epoch 164/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 338.4484 - val_loss: 367.1792\n",
      "Epoch 165/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 335.2623 - val_loss: 366.1763\n",
      "Epoch 166/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 332.7858 - val_loss: 362.6204\n",
      "Epoch 167/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 336.1241 - val_loss: 358.2199\n",
      "Epoch 168/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 326.2900 - val_loss: 354.2570\n",
      "Epoch 169/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 322.1819 - val_loss: 353.2215\n",
      "Epoch 170/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 321.0921 - val_loss: 351.0940\n",
      "Epoch 171/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 318.7063 - val_loss: 349.3665\n",
      "Epoch 172/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 315.8859 - val_loss: 347.3202\n",
      "Epoch 173/350\n",
      "387/387 [==============================] - 0s 146us/step - loss: 314.8978 - val_loss: 344.0177\n",
      "Epoch 174/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 310.0902 - val_loss: 340.2742\n",
      "Epoch 175/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 313.4881 - val_loss: 337.5864\n",
      "Epoch 176/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 306.1254 - val_loss: 335.1752\n",
      "Epoch 177/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 305.2627 - val_loss: 333.5207\n",
      "Epoch 178/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 301.5658 - val_loss: 331.4150\n",
      "Epoch 179/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 300.4941 - val_loss: 328.6894\n",
      "Epoch 180/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 296.5634 - val_loss: 324.2011\n",
      "Epoch 181/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 296.6356 - val_loss: 319.3248\n",
      "Epoch 182/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 290.4248 - val_loss: 314.2151\n",
      "Epoch 183/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 285.2549 - val_loss: 312.3104\n",
      "Epoch 184/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 286.5467 - val_loss: 310.0494\n",
      "Epoch 185/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 284.2355 - val_loss: 306.9161\n",
      "Epoch 186/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 279.1836 - val_loss: 304.4865\n",
      "Epoch 187/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 276.9774 - val_loss: 303.0621\n",
      "Epoch 188/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 273.5025 - val_loss: 301.1892\n",
      "Epoch 189/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 275.3449 - val_loss: 298.2293\n",
      "Epoch 190/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 269.3512 - val_loss: 293.8982\n",
      "Epoch 191/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 265.9182 - val_loss: 291.5838\n",
      "Epoch 192/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 265.8648 - val_loss: 289.6534\n",
      "Epoch 193/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 267.2721 - val_loss: 287.6374\n",
      "Epoch 194/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 260.7709 - val_loss: 287.6635\n",
      "Epoch 195/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 258.8112 - val_loss: 287.5573\n",
      "Epoch 196/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 257.1886 - val_loss: 284.4308\n",
      "Epoch 197/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 255.3155 - val_loss: 281.9805\n",
      "Epoch 198/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 251.5315 - val_loss: 278.4290\n",
      "Epoch 199/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 249.9366 - val_loss: 274.1564\n",
      "Epoch 200/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 249.0572 - val_loss: 272.0706\n",
      "Epoch 201/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 246.7652 - val_loss: 269.6316\n",
      "Epoch 202/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 243.2323 - val_loss: 267.2901\n",
      "Epoch 203/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 240.4070 - val_loss: 266.8919\n",
      "Epoch 204/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 244.4779 - val_loss: 267.3855\n",
      "Epoch 205/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 245.0051 - val_loss: 265.0016\n",
      "Epoch 206/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 242.9216 - val_loss: 261.0071\n",
      "Epoch 207/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 240.1406 - val_loss: 258.0109\n",
      "Epoch 208/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 232.2168 - val_loss: 258.2226\n",
      "Epoch 209/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 232.7338 - val_loss: 255.7016\n",
      "Epoch 210/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 227.8325 - val_loss: 250.4541\n",
      "Epoch 211/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 224.8638 - val_loss: 247.9938\n",
      "Epoch 212/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 220.4734 - val_loss: 245.9205\n",
      "Epoch 213/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 222.1466 - val_loss: 244.2251\n",
      "Epoch 214/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 221.3193 - val_loss: 242.2005\n",
      "Epoch 215/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 217.4014 - val_loss: 242.2180\n",
      "Epoch 216/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 217.3653 - val_loss: 243.4780\n",
      "Epoch 217/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 220.7162 - val_loss: 242.4178\n",
      "Epoch 218/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 217.1449 - val_loss: 239.0248\n",
      "Epoch 219/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 214.2599 - val_loss: 234.9695\n",
      "Epoch 220/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 210.6211 - val_loss: 231.9683\n",
      "Epoch 221/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 206.2284 - val_loss: 230.5392\n",
      "Epoch 222/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 206.1836 - val_loss: 229.4549\n",
      "Epoch 223/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 204.9582 - val_loss: 228.8015\n",
      "Epoch 224/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 201.6261 - val_loss: 228.8792\n",
      "Epoch 225/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 203.8555 - val_loss: 227.5015\n",
      "Epoch 226/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 197.4614 - val_loss: 225.3403\n",
      "Epoch 227/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 202.9705 - val_loss: 221.7405\n",
      "Epoch 228/350\n",
      "387/387 [==============================] - 0s 157us/step - loss: 195.4535 - val_loss: 218.6879\n",
      "Epoch 229/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 195.2309 - val_loss: 217.0563\n",
      "Epoch 230/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 193.1738 - val_loss: 216.2607\n",
      "Epoch 231/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 192.3725 - val_loss: 216.9313\n",
      "Epoch 232/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 188.3406 - val_loss: 220.4401\n",
      "Epoch 233/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 191.4092 - val_loss: 222.3809\n",
      "Epoch 234/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 192.6196 - val_loss: 219.0611\n",
      "Epoch 235/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 186.7589 - val_loss: 213.4177\n",
      "Epoch 236/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 185.1490 - val_loss: 208.3536\n",
      "Epoch 237/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 181.7303 - val_loss: 205.6132\n",
      "Epoch 238/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 183.5986 - val_loss: 203.8911\n",
      "Epoch 239/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 179.8257 - val_loss: 202.3722\n",
      "Epoch 240/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 176.8327 - val_loss: 201.9964\n",
      "Epoch 241/350\n",
      "387/387 [==============================] - 0s 157us/step - loss: 176.3336 - val_loss: 202.6314\n",
      "Epoch 242/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 176.9722 - val_loss: 202.1174\n",
      "Epoch 243/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 174.6524 - val_loss: 200.7025\n",
      "Epoch 244/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 172.0582 - val_loss: 199.9747\n",
      "Epoch 245/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 173.4270 - val_loss: 198.9543\n",
      "Epoch 246/350\n",
      "387/387 [==============================] - 0s 162us/step - loss: 172.6075 - val_loss: 197.8896\n",
      "Epoch 247/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 170.3458 - val_loss: 195.4782\n",
      "Epoch 248/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 168.1994 - val_loss: 193.5180\n",
      "Epoch 249/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 168.1266 - val_loss: 192.6972\n",
      "Epoch 250/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 166.6646 - val_loss: 192.5698\n",
      "Epoch 251/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 168.9960 - val_loss: 191.4849\n",
      "Epoch 252/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 166.5288 - val_loss: 188.2169\n",
      "Epoch 253/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 162.9549 - val_loss: 186.2092\n",
      "Epoch 254/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 162.1361 - val_loss: 185.9468\n",
      "Epoch 255/350\n",
      "387/387 [==============================] - 0s 152us/step - loss: 163.6802 - val_loss: 186.8349\n",
      "Epoch 256/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 160.4425 - val_loss: 187.0132\n",
      "Epoch 257/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 158.8153 - val_loss: 186.6032\n",
      "Epoch 258/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 158.3795 - val_loss: 184.2307\n",
      "Epoch 259/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 155.4555 - val_loss: 183.2157\n",
      "Epoch 260/350\n",
      "387/387 [==============================] - 0s 160us/step - loss: 156.1388 - val_loss: 182.8655\n",
      "Epoch 261/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 154.9084 - val_loss: 181.1973\n",
      "Epoch 262/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 153.8886 - val_loss: 178.4962\n",
      "Epoch 263/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 151.5933 - val_loss: 177.1187\n",
      "Epoch 264/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 151.3880 - val_loss: 175.7941\n",
      "Epoch 265/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 150.4687 - val_loss: 174.7240\n",
      "Epoch 266/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 149.0179 - val_loss: 173.2518\n",
      "Epoch 267/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 147.1833 - val_loss: 171.6073\n",
      "Epoch 268/350\n",
      "387/387 [==============================] - 0s 139us/step - loss: 147.6736 - val_loss: 170.7291\n",
      "Epoch 269/350\n",
      "387/387 [==============================] - 0s 138us/step - loss: 147.9504 - val_loss: 170.3079\n",
      "Epoch 270/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 145.2682 - val_loss: 169.2916\n",
      "Epoch 271/350\n",
      "387/387 [==============================] - 0s 155us/step - loss: 143.4443 - val_loss: 167.8393\n",
      "Epoch 272/350\n",
      "387/387 [==============================] - 0s 175us/step - loss: 142.6075 - val_loss: 166.3572\n",
      "Epoch 273/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 142.3440 - val_loss: 165.5130\n",
      "Epoch 274/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 142.4107 - val_loss: 165.0937\n",
      "Epoch 275/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 139.3159 - val_loss: 165.4938\n",
      "Epoch 276/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 142.7631 - val_loss: 165.1460\n",
      "Epoch 277/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 140.0431 - val_loss: 163.1242\n",
      "Epoch 278/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 139.4695 - val_loss: 160.5215\n",
      "Epoch 279/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 137.0854 - val_loss: 159.7930\n",
      "Epoch 280/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 135.8883 - val_loss: 159.6296\n",
      "Epoch 281/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 136.7110 - val_loss: 159.2940\n",
      "Epoch 282/350\n",
      "387/387 [==============================] - 0s 142us/step - loss: 134.7879 - val_loss: 158.2574\n",
      "Epoch 283/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 135.6029 - val_loss: 157.1144\n",
      "Epoch 284/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 134.8152 - val_loss: 155.1773\n",
      "Epoch 285/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 131.8129 - val_loss: 154.8231\n",
      "Epoch 286/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 131.9349 - val_loss: 154.4834\n",
      "Epoch 287/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 131.3286 - val_loss: 152.9206\n",
      "Epoch 288/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 130.0355 - val_loss: 152.8607\n",
      "Epoch 289/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 130.5408 - val_loss: 153.5242\n",
      "Epoch 290/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 129.4764 - val_loss: 153.9716\n",
      "Epoch 291/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 129.4763 - val_loss: 153.9586\n",
      "Epoch 292/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 128.3126 - val_loss: 154.2773\n",
      "Epoch 293/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 129.2846 - val_loss: 152.9151\n",
      "Epoch 294/350\n",
      "387/387 [==============================] - 0s 132us/step - loss: 126.4988 - val_loss: 150.1347\n",
      "Epoch 295/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 126.0315 - val_loss: 149.2708\n",
      "Epoch 296/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 124.0934 - val_loss: 149.0320\n",
      "Epoch 297/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 124.0828 - val_loss: 148.3898\n",
      "Epoch 298/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 123.2753 - val_loss: 147.2215\n",
      "Epoch 299/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 122.7219 - val_loss: 145.7305\n",
      "Epoch 300/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 0s 152us/step - loss: 123.1007 - val_loss: 145.2414\n",
      "Epoch 301/350\n",
      "387/387 [==============================] - 0s 149us/step - loss: 120.7817 - val_loss: 146.2963\n",
      "Epoch 302/350\n",
      "387/387 [==============================] - 0s 147us/step - loss: 121.0318 - val_loss: 146.4183\n",
      "Epoch 303/350\n",
      "387/387 [==============================] - 0s 144us/step - loss: 120.4623 - val_loss: 145.2220\n",
      "Epoch 304/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 119.7949 - val_loss: 143.7581\n",
      "Epoch 305/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 119.7674 - val_loss: 142.7851\n",
      "Epoch 306/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 118.3891 - val_loss: 142.6415\n",
      "Epoch 307/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 122.1408 - val_loss: 143.9606\n",
      "Epoch 308/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 117.2498 - val_loss: 144.5409\n",
      "Epoch 309/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 117.8270 - val_loss: 143.9364\n",
      "Epoch 310/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 116.8752 - val_loss: 140.3886\n",
      "Epoch 311/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 115.3179 - val_loss: 139.8705\n",
      "Epoch 312/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 117.2106 - val_loss: 139.8925\n",
      "Epoch 313/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 115.9452 - val_loss: 139.9671\n",
      "Epoch 314/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 113.2254 - val_loss: 142.0533\n",
      "Epoch 315/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 113.1472 - val_loss: 144.8999\n",
      "Epoch 316/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 114.9250 - val_loss: 144.0796\n",
      "Epoch 317/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 114.5872 - val_loss: 141.3745\n",
      "Epoch 318/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 114.0773 - val_loss: 140.2420\n",
      "Epoch 319/350\n",
      "387/387 [==============================] - 0s 133us/step - loss: 114.6862 - val_loss: 138.9675\n",
      "Epoch 320/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 111.9400 - val_loss: 138.5831\n",
      "Epoch 321/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 110.2190 - val_loss: 142.3220\n",
      "Epoch 322/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 112.6700 - val_loss: 147.1937\n",
      "Epoch 323/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 114.1826 - val_loss: 147.5317\n",
      "Epoch 324/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 114.0614 - val_loss: 145.0564\n",
      "Epoch 325/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 110.0380 - val_loss: 141.7646\n",
      "Epoch 326/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 108.3415 - val_loss: 139.9509\n",
      "Epoch 327/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 109.7646 - val_loss: 138.8052\n",
      "Epoch 328/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 109.8461 - val_loss: 137.4583\n",
      "Epoch 329/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 105.9427 - val_loss: 135.4467\n",
      "Epoch 330/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 107.0044 - val_loss: 134.5255\n",
      "Epoch 331/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 105.1728 - val_loss: 135.0393\n",
      "Epoch 332/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 108.1365 - val_loss: 135.6173\n",
      "Epoch 333/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 106.6377 - val_loss: 136.0847\n",
      "Epoch 334/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 104.9348 - val_loss: 137.2890\n",
      "Epoch 335/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 106.5220 - val_loss: 137.0706\n",
      "Epoch 336/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 106.4313 - val_loss: 137.0710\n",
      "Epoch 337/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 104.1979 - val_loss: 139.6551\n",
      "Epoch 338/350\n",
      "387/387 [==============================] - 0s 134us/step - loss: 103.9504 - val_loss: 144.4979\n",
      "Epoch 339/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 103.9075 - val_loss: 143.8236\n",
      "Epoch 340/350\n",
      "387/387 [==============================] - 0s 137us/step - loss: 105.3214 - val_loss: 138.8707\n",
      "Epoch 341/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 104.5847 - val_loss: 134.6851\n",
      "Epoch 342/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 101.9181 - val_loss: 131.6960\n",
      "Epoch 343/350\n",
      "387/387 [==============================] - 0s 129us/step - loss: 104.2783 - val_loss: 130.4229\n",
      "Epoch 344/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 102.6280 - val_loss: 128.7799\n",
      "Epoch 345/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 102.4336 - val_loss: 128.4352\n",
      "Epoch 346/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 102.1948 - val_loss: 129.5633\n",
      "Epoch 347/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 102.3743 - val_loss: 129.0106\n",
      "Epoch 348/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 101.1817 - val_loss: 127.0599\n",
      "Epoch 349/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 101.5603 - val_loss: 127.6067\n",
      "Epoch 350/350\n",
      "387/387 [==============================] - 0s 131us/step - loss: 102.5064 - val_loss: 131.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a18837b0b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型拟合与评估\n",
    "finish_model.fit([X_train,X_HA_train], label_train, epochs=350, batch_size=128,\n",
    "validation_data=([image_test,day_test], label_test))\n",
    "# finish_model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型预测\n",
    "model_pre = finish_model.predict([image_test,day_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape = 0.16848346268922584\n",
      "smape = 0.16875046142067715\n",
      "mae = 8.396290082731765\n"
     ]
    }
   ],
   "source": [
    "#计算各项误差指标\n",
    "\n",
    "mape_mean = mape_loss_func(model_pre, label_test)\n",
    "smape_mean = smape_loss_func(model_pre, label_test)\n",
    "mae_mean = mae_loss_func(model_pre, label_test)\n",
    "\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存\n",
    "finish_model.save_weights('../model/base_ST-DTNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th link\n",
      "0.20479696802046068\n",
      "2th link\n",
      "0.14227843974406434\n",
      "3th link\n",
      "0.2650269575123997\n",
      "4th link\n",
      "0.1594440468144884\n",
      "5th link\n",
      "0.08932931679937321\n",
      "6th link\n",
      "0.46309688178963576\n",
      "7th link\n",
      "0.12618120493255244\n",
      "8th link\n",
      "0.1739794072964205\n",
      "9th link\n",
      "0.15485085285216854\n",
      "10th link\n",
      "0.11537330598072179\n",
      "11th link\n",
      "0.13560026141067966\n",
      "12th link\n",
      "0.11719991107622361\n",
      "13th link\n",
      "0.07927230609617741\n",
      "14th link\n",
      "0.09964801629170982\n",
      "15th link\n",
      "0.1683458918451683\n",
      "16th link\n",
      "0.10664279511241108\n",
      "17th link\n",
      "0.0761571312419101\n",
      "18th link\n",
      "0.24057246162817622\n",
      "19th link\n",
      "0.1669896688545757\n",
      "20th link\n",
      "0.15410513237112258\n",
      "21th link\n",
      "0.1452593316602408\n",
      "22th link\n",
      "0.17074323618097872\n",
      "23th link\n",
      "0.32046905810877613\n",
      "24th link\n",
      "0.2500891980557945\n",
      "25th link\n",
      "0.10009973309989077\n"
     ]
    }
   ],
   "source": [
    "#计算每条路段的误差\n",
    "mape_list = []\n",
    "for i in range(num_links):\n",
    "    a1 = mape_loss_func(model_pre[:,i,:], label_test[:,i,:])\n",
    "    mape_list.append(a1)\n",
    "    print(str(i+1)+'th link')\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mape_pd = pd.Series(mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16    0.076157\n",
       "12    0.079272\n",
       "4     0.089329\n",
       "13    0.099648\n",
       "24    0.100100\n",
       "15    0.106643\n",
       "9     0.115373\n",
       "11    0.117200\n",
       "6     0.126181\n",
       "10    0.135600\n",
       "1     0.142278\n",
       "20    0.145259\n",
       "19    0.154105\n",
       "8     0.154851\n",
       "3     0.159444\n",
       "18    0.166990\n",
       "14    0.168346\n",
       "21    0.170743\n",
       "7     0.173979\n",
       "0     0.204797\n",
       "17    0.240572\n",
       "23    0.250089\n",
       "2     0.265027\n",
       "22    0.320469\n",
       "5     0.463097\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_pd.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
