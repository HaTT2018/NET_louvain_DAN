{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss_func(preds, labels):\n",
    "    mask = labels > 5\n",
    "    return np.mean(np.fabs(labels[mask]-preds[mask])/labels[mask])\n",
    "\n",
    "def smape_loss_func(preds, labels):\n",
    "    mask= labels > 5\n",
    "    return np.mean(2*np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\n",
    "\n",
    "def mae_loss_func(preds, labels):\n",
    "    mask= labels > 5\n",
    "    return np.fabs((labels[mask]-preds[mask])).mean()\n",
    "\n",
    "def eliminate_nan(b):\n",
    "    a = np.array(b)\n",
    "    c = a[~np.isnan(a)]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 class(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "randseed = 25\n",
    "res = 11\n",
    "\n",
    "v = pd.read_csv('../data/v_20_aggragated.csv')\n",
    "v = v.rename(columns={'Unnamed: 0': 'id'})\n",
    "det_with_class = pd.read_csv('../res/%i_res%i_id_402_withclass.csv'%(randseed, res), index_col=0)\n",
    "\n",
    "v['class_i'] = ''\n",
    "for i in range(len(v)):\n",
    "    v.loc[i, 'class_i'] = det_with_class[det_with_class['id']==v.loc[i, 'id']].iloc[0, 5]  # 5 stands for 'class_i'\n",
    "\n",
    "num_class = det_with_class['class_i'].drop_duplicates().size\n",
    "\n",
    "v_class = []\n",
    "for i in range(num_class):\n",
    "    v_class.append(v[v['class_i']==i])\n",
    "\n",
    "print('There are %i class(es)'%num_class)\n",
    "\n",
    "dist_mat = pd.read_csv('../data/dist_mat.csv', index_col=0)\n",
    "id_info = pd.read_csv('../data/id2000.csv', index_col=0)\n",
    "dist_mat.index = id_info['id2']\n",
    "dist_mat.columns = id_info['id2']\n",
    "for i in range(len(dist_mat)):\n",
    "    for j in range(len(dist_mat)):\n",
    "        if i==j:\n",
    "            dist_mat.iloc[i, j] = 0\n",
    "\n",
    "near_id = pd.DataFrame(np.argsort(np.array(dist_mat)), index = id_info['id2'], columns = id_info['id2'])\n",
    "\n",
    "def get_node(det, seg):\n",
    "    # det is one single detector id\n",
    "    # node is one single node id\n",
    "    \n",
    "    # seg = pd.read_csv('./data/segement.csv', header=None)\n",
    "    try:\n",
    "        node_info = seg[seg[6]==det]\n",
    "        node = node_info.iloc[0, 0]\n",
    "    except:\n",
    "        node_info = seg[seg[7]==det]\n",
    "        node = node_info.iloc[0, 0]\n",
    "        \n",
    "    return node\n",
    "\n",
    "def get_class_with_node(seg, v_class):\n",
    "    det_list_class = np.array([])\n",
    "    try:\n",
    "        v_class.insert(1, 'id2', '')  # id2 mean node id\n",
    "    except:\n",
    "        v_class['id2'] = ''\n",
    "        \n",
    "    for i in range(len(v_class)):\n",
    "        det_list_class = np.append(det_list_class, v_class.iloc[i, 0])\n",
    "        v_class.iloc[i, 1] = get_node(v_class.iloc[i, 0], seg)\n",
    "    \n",
    "    return det_list_class, v_class\n",
    "\n",
    "def rds_mat(old_dist_mat, det_ids):\n",
    "    # get a matrix that contains n raods that have specified node id s\n",
    "    node_ids = np.array([])\n",
    "    for i in det_ids:\n",
    "        node_ids = np.append(node_ids, get_node(i, seg))\n",
    "        \n",
    "    new_dist_mat = old_dist_mat.loc[node_ids, node_ids]\n",
    "    old_dist_mat = np.array(old_dist_mat)\n",
    "    new_near_id_mat = np.argsort(new_dist_mat)\n",
    "    return new_near_id_mat\n",
    "\n",
    "seg = pd.read_csv('../data/segement.csv', header=None)\n",
    "num_dets = 25\n",
    "\n",
    "det_list_class = []\n",
    "for i in range(num_class):\n",
    "    det_list_class_temp, v_class_temp = get_class_with_node(seg, v_class[i])\n",
    "    det_list_class.append(det_list_class_temp)\n",
    "    v_class_temp = v_class_temp[v_class_temp['id'].isin(det_list_class_temp[:num_dets])]\n",
    "    v_class[i] = v_class_temp\n",
    "    \n",
    "near_road_set = []\n",
    "for i in range(num_class):\n",
    "    near_road_set.append(rds_mat(dist_mat, det_list_class[i][:num_dets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(flow, near_road, from_day, to_day, prop):\n",
    "    flow = np.array(flow)\n",
    "    # 选数据\n",
    "    flow = flow[:, 144*(from_day-1):144*(to_day)]\n",
    "    print(flow.shape)\n",
    "\n",
    "    # 利用滑动窗口的方式，重构数据为(n，最近路段数，输入时间窗，总路段数)的形式\n",
    "\n",
    "    global k, t_p, t_input, t_pre, num_links\n",
    "    k = 5 # 参数k为需考虑的最近路段数\n",
    "    t_p = 10 # 参数t_p为总时间序列长度（天）\n",
    "    t_input = 12 #参数t_input为输入时间窗(10min颗粒度)\n",
    "    t_pre = 3 #参数t_pre为预测时间窗(10min颗粒度)\n",
    "    num_links = 25 #参数num_links为总路段数\n",
    "\n",
    "\n",
    "    image = []\n",
    "    for i in range(np.shape(near_road)[0]):\n",
    "        road_id = []\n",
    "        for j in range(k):\n",
    "            road_id.append(near_road[i][j])\n",
    "        image.append(flow[road_id, :])\n",
    "    image1 = np.reshape(image, [-1, k, len(flow[0,:])])\n",
    "    image2 = np.transpose(image1,(1,2,0))\n",
    "    image3 = []\n",
    "    label = []\n",
    "    day = []\n",
    "\n",
    "    for i in range(1, t_p):\n",
    "        for j in range(144-t_input-t_pre):\n",
    "            image3.append(image2[:, i*144+j:i*144+j+t_input, :][:])\n",
    "            label.append(flow[:, i*144+j+t_input:i*144+j+t_input+t_pre][:])\n",
    "            day.append(flow[:, (i-1)*144+j+t_input:(i-1)*144+j+t_input+t_pre][:])\n",
    "\n",
    "    image3 = np.asarray(image3)\n",
    "    label = np.asarray(label)\n",
    "    day =  np.asarray(day)\n",
    "\n",
    "    print(np.shape(image3))\n",
    "\n",
    "    #划分前90%数据为训练集，最后10%数据为测试集\n",
    "    image_train = image3[:int(np.shape(image3)[0]*prop)]\n",
    "    image_test = image3[int(np.shape(image3)[0]*prop):]\n",
    "    label_train = label[:int(np.shape(label)[0]*prop)]\n",
    "    label_test = label[int(np.shape(label)[0]*prop):]\n",
    "\n",
    "    day_train = day[:int(np.shape(day)[0]*prop)]\n",
    "    day_test = day[int(np.shape(day)[0]*prop):]\n",
    "\n",
    "    print(image_train.shape)\n",
    "    print(image_test.shape)\n",
    "    print(label_train.shape)\n",
    "    print(label_test.shape)\n",
    "    \n",
    "    return image_train, image_test, day_train, day_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4464)\n",
      "(1161, 5, 12, 25)\n",
      "(898, 5, 12, 25)\n",
      "(263, 5, 12, 25)\n",
      "(898, 25, 3)\n",
      "(263, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "# ind, class\n",
    "# 0  , blue\n",
    "# 1  , green\n",
    "# 2  , yellow  <--\n",
    "# 3  , black   <--\n",
    "# 4  , red     <--\n",
    "class_color_set = ['b', 'g', 'y', 'black', 'r']\n",
    "class_i = 2\n",
    "\n",
    "near_road = np.array(near_road_set[class_i])\n",
    "flow = v_class[class_i].iloc[:, 2:-1]\n",
    "\n",
    "prop = 24/31  # proportion of training data\n",
    "\n",
    "image_train, image_test, day_train, day_test, label_train, label_test\\\n",
    "= sliding_window(\n",
    "    flow, near_road, from_day=1, to_day=31, prop=prop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define merge layer\n",
    "class Merge_Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Merge_Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.para1 = self.add_weight(shape=(input_shape[0][1], input_shape[0][2]),\n",
    "                                     initializer='uniform', trainable=True,\n",
    "                                     name='para1')\n",
    "        self.para2 = self.add_weight(shape=(input_shape[1][1], input_shape[1][2]),\n",
    "                                     initializer='uniform', trainable=True,\n",
    "                                     name='para2')\n",
    "        super(Merge_Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mat1 = inputs[0]\n",
    "        mat2 = inputs[1]\n",
    "        output = mat1 * self.para1 + mat2 * self.para2\n",
    "        # output = mat1 * 0.1 + mat2 * 0.9\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 5, 12, 25)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 12, 25)    100         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 12, 25)    5650        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 12, 25)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 12, 25)    100         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 12, 25)    5650        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 5, 12, 25)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1500)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1500)         6000        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1500)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 150)          225150      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 75)           11325       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 25, 3)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_HA (InputLayer)           (None, 25, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge__layer_1 (Merge_Layer)    (None, 25, 3)        150         reshape_1[0][0]                  \n",
      "                                                                 input_HA[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 254,125\n",
      "Trainable params: 251,025\n",
      "Non-trainable params: 3,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = keras.Input(shape=(k,t_input,num_links), name='input_data')\n",
    "input_HA = keras.Input(shape=(num_links, t_pre), name='input_HA')\n",
    "\n",
    "x = keras.layers.BatchNormalization(input_shape =(k,t_input,num_links))(input_data)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                           filters = num_links,\n",
    "                           kernel_size = 3,\n",
    "                           strides = 1,\n",
    "                           padding=\"SAME\",\n",
    "                           activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "                       filters = num_links,\n",
    "                       kernel_size = 3,\n",
    "                       strides = 1,\n",
    "                       padding=\"SAME\",\n",
    "                       activation='relu')(x)\n",
    "\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\n",
    "                                strides = 1,\n",
    "                                padding = \"SAME\",\n",
    "                                )(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = keras.layers.Dense(num_links*2*t_pre, activation='relu', name='dense_1')(x)\n",
    "x = keras.layers.Dense(num_links*t_pre, activation='relu', name='dense_2')(x)\n",
    "\n",
    "output = keras.layers.Reshape((num_links,t_pre))(x)\n",
    "\n",
    "output_final = Merge_Layer()([output, input_HA])\n",
    "\n",
    "# construct model\n",
    "finish_model = keras.models.Model([input_data,input_HA], [output_final])\n",
    "\n",
    "finish_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finish_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = image_train\n",
    "X_HA_train = day_train\n",
    "label_train = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 898 samples, validate on 263 samples\n",
      "Epoch 1/100\n",
      "898/898 [==============================] - 1s 1ms/step - loss: 4057.3298 - val_loss: 3901.6988\n",
      "Epoch 2/100\n",
      "898/898 [==============================] - 0s 86us/step - loss: 3978.8333 - val_loss: 3784.0184\n",
      "Epoch 3/100\n",
      "898/898 [==============================] - 0s 88us/step - loss: 3871.0251 - val_loss: 3602.8735\n",
      "Epoch 4/100\n",
      "898/898 [==============================] - 0s 92us/step - loss: 3704.2863 - val_loss: 3329.2337\n",
      "Epoch 5/100\n",
      "898/898 [==============================] - 0s 89us/step - loss: 3436.2273 - val_loss: 2941.6293\n",
      "Epoch 6/100\n",
      "898/898 [==============================] - 0s 87us/step - loss: 3047.9091 - val_loss: 2506.9249\n",
      "Epoch 7/100\n",
      "898/898 [==============================] - 0s 89us/step - loss: 2580.8703 - val_loss: 2229.8790\n",
      "Epoch 8/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 2181.8227 - val_loss: 2205.3202\n",
      "Epoch 9/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 1992.9389 - val_loss: 2077.1487\n",
      "Epoch 10/100\n",
      "898/898 [==============================] - 0s 98us/step - loss: 1879.4745 - val_loss: 1989.8114\n",
      "Epoch 11/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 1773.8206 - val_loss: 1871.8438\n",
      "Epoch 12/100\n",
      "898/898 [==============================] - 0s 86us/step - loss: 1708.9061 - val_loss: 1822.8694\n",
      "Epoch 13/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 1653.3856 - val_loss: 1904.5420\n",
      "Epoch 14/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 1598.1939 - val_loss: 1722.8001\n",
      "Epoch 15/100\n",
      "898/898 [==============================] - 0s 76us/step - loss: 1548.3489 - val_loss: 1626.8288\n",
      "Epoch 16/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 1495.6489 - val_loss: 1628.0009\n",
      "Epoch 17/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 1465.3587 - val_loss: 1466.0604\n",
      "Epoch 18/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 1415.4694 - val_loss: 1385.5218\n",
      "Epoch 19/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 1386.7899 - val_loss: 1382.0837\n",
      "Epoch 20/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 1356.2980 - val_loss: 1384.8672\n",
      "Epoch 21/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 1341.9321 - val_loss: 1342.7347\n",
      "Epoch 22/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 1306.4998 - val_loss: 1311.1942\n",
      "Epoch 23/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 1287.8300 - val_loss: 1308.4195\n",
      "Epoch 24/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 1263.5556 - val_loss: 1382.4408\n",
      "Epoch 25/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 1251.8287 - val_loss: 1231.9110\n",
      "Epoch 26/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 1208.9146 - val_loss: 1165.5353\n",
      "Epoch 27/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 1162.3066 - val_loss: 1125.8298\n",
      "Epoch 28/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 1113.1137 - val_loss: 1095.1525\n",
      "Epoch 29/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 1080.2661 - val_loss: 1064.8022\n",
      "Epoch 30/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 1054.4467 - val_loss: 1037.5672\n",
      "Epoch 31/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 1043.2523 - val_loss: 1012.7185\n",
      "Epoch 32/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 1009.2489 - val_loss: 1014.9703\n",
      "Epoch 33/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 994.0703 - val_loss: 968.1347\n",
      "Epoch 34/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 969.9230 - val_loss: 935.5805\n",
      "Epoch 35/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 941.5723 - val_loss: 946.9983\n",
      "Epoch 36/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 924.2313 - val_loss: 982.9791\n",
      "Epoch 37/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 891.7727 - val_loss: 878.8931\n",
      "Epoch 38/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 865.6553 - val_loss: 887.0109\n",
      "Epoch 39/100\n",
      "898/898 [==============================] - 0s 85us/step - loss: 836.3450 - val_loss: 885.3805\n",
      "Epoch 40/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 820.1554 - val_loss: 807.5978\n",
      "Epoch 41/100\n",
      "898/898 [==============================] - 0s 86us/step - loss: 799.5859 - val_loss: 776.4211\n",
      "Epoch 42/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 784.2485 - val_loss: 765.5722\n",
      "Epoch 43/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 769.5552 - val_loss: 768.5693\n",
      "Epoch 44/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 763.6363 - val_loss: 745.8370\n",
      "Epoch 45/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 744.0934 - val_loss: 743.1278\n",
      "Epoch 46/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 733.8211 - val_loss: 742.0724\n",
      "Epoch 47/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 714.3378 - val_loss: 693.9869\n",
      "Epoch 48/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 701.7594 - val_loss: 688.1154\n",
      "Epoch 49/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 692.2775 - val_loss: 687.5716\n",
      "Epoch 50/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 680.4842 - val_loss: 669.9574\n",
      "Epoch 51/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 663.6904 - val_loss: 650.6515\n",
      "Epoch 52/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 655.3287 - val_loss: 634.3902\n",
      "Epoch 53/100\n",
      "898/898 [==============================] - 0s 81us/step - loss: 641.8146 - val_loss: 621.8351\n",
      "Epoch 54/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 634.0407 - val_loss: 610.9100\n",
      "Epoch 55/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 615.2155 - val_loss: 600.8402\n",
      "Epoch 56/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 606.2159 - val_loss: 582.0907\n",
      "Epoch 57/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 596.7117 - val_loss: 578.5482\n",
      "Epoch 58/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 584.3329 - val_loss: 565.5894\n",
      "Epoch 59/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 571.2903 - val_loss: 550.5686\n",
      "Epoch 60/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 560.9746 - val_loss: 541.1201\n",
      "Epoch 61/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 557.2696 - val_loss: 525.5750\n",
      "Epoch 62/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 542.8503 - val_loss: 523.3619\n",
      "Epoch 63/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 532.7898 - val_loss: 514.4766\n",
      "Epoch 64/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 522.2822 - val_loss: 497.4655\n",
      "Epoch 65/100\n",
      "898/898 [==============================] - 0s 76us/step - loss: 521.2382 - val_loss: 486.2517\n",
      "Epoch 66/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 506.6884 - val_loss: 476.4346\n",
      "Epoch 67/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 494.8741 - val_loss: 473.4364\n",
      "Epoch 68/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 488.7093 - val_loss: 462.5866\n",
      "Epoch 69/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 478.9202 - val_loss: 451.7103\n",
      "Epoch 70/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 472.1303 - val_loss: 444.6994\n",
      "Epoch 71/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 461.7650 - val_loss: 437.3324\n",
      "Epoch 72/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 458.6743 - val_loss: 429.0163\n",
      "Epoch 73/100\n",
      "898/898 [==============================] - 0s 85us/step - loss: 451.8444 - val_loss: 414.3670\n",
      "Epoch 74/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 434.7198 - val_loss: 397.7264\n",
      "Epoch 75/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 412.2635 - val_loss: 384.6168\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 0s 80us/step - loss: 408.6724 - val_loss: 389.9944\n",
      "Epoch 77/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 401.2798 - val_loss: 378.8777\n",
      "Epoch 78/100\n",
      "898/898 [==============================] - ETA: 0s - loss: 394.421 - 0s 76us/step - loss: 394.8323 - val_loss: 364.6513\n",
      "Epoch 79/100\n",
      "898/898 [==============================] - 0s 82us/step - loss: 387.0577 - val_loss: 355.2471\n",
      "Epoch 80/100\n",
      "898/898 [==============================] - 0s 83us/step - loss: 380.4459 - val_loss: 349.0973\n",
      "Epoch 81/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 373.0723 - val_loss: 342.7726\n",
      "Epoch 82/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 362.7571 - val_loss: 336.3631\n",
      "Epoch 83/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 356.2508 - val_loss: 328.5857\n",
      "Epoch 84/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 349.3915 - val_loss: 322.1546\n",
      "Epoch 85/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 344.6656 - val_loss: 316.0924\n",
      "Epoch 86/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 338.4722 - val_loss: 315.1234\n",
      "Epoch 87/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 333.1277 - val_loss: 305.9303\n",
      "Epoch 88/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 324.1372 - val_loss: 298.4820\n",
      "Epoch 89/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 318.0246 - val_loss: 292.8078\n",
      "Epoch 90/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 317.4284 - val_loss: 288.0933\n",
      "Epoch 91/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 310.8339 - val_loss: 282.7979\n",
      "Epoch 92/100\n",
      "898/898 [==============================] - 0s 78us/step - loss: 301.3451 - val_loss: 282.8374\n",
      "Epoch 93/100\n",
      "898/898 [==============================] - 0s 77us/step - loss: 299.9596 - val_loss: 272.9848\n",
      "Epoch 94/100\n",
      "898/898 [==============================] - 0s 84us/step - loss: 295.8529 - val_loss: 268.0458\n",
      "Epoch 95/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 286.6664 - val_loss: 269.4129\n",
      "Epoch 96/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 283.0923 - val_loss: 265.5612\n",
      "Epoch 97/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 279.4677 - val_loss: 250.6619\n",
      "Epoch 98/100\n",
      "898/898 [==============================] - 0s 80us/step - loss: 272.7171 - val_loss: 246.0947\n",
      "Epoch 99/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 266.3364 - val_loss: 247.0751\n",
      "Epoch 100/100\n",
      "898/898 [==============================] - 0s 79us/step - loss: 263.4698 - val_loss: 242.7296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183e1d49198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型拟合与评估\n",
    "finish_model.fit([X_train,X_HA_train], label_train, epochs=100, batch_size=128,\n",
    "validation_data=([image_test,day_test], label_test))\n",
    "# finish_model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型预测\n",
    "model_pre = finish_model.predict([image_test,day_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape = 0.23551441632811373\n",
      "smape = 0.2438822545933385\n",
      "mae = 11.871502050023368\n"
     ]
    }
   ],
   "source": [
    "#计算各项误差指标\n",
    "\n",
    "mape_mean = mape_loss_func(model_pre, label_test)\n",
    "smape_mean = smape_loss_func(model_pre, label_test)\n",
    "mae_mean = mae_loss_func(model_pre, label_test)\n",
    "\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存\n",
    "finish_model.save_weights('../model/source_%s.h5'%class_color_set[class_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th link\n",
      "0.26261731765667157\n",
      "2th link\n",
      "0.39090020482780297\n",
      "3th link\n",
      "0.1534360003968672\n",
      "4th link\n",
      "0.07588254645986028\n",
      "5th link\n",
      "0.19515292140225413\n",
      "6th link\n",
      "0.1647863351166111\n",
      "7th link\n",
      "0.30753262023317013\n",
      "8th link\n",
      "0.3324902440080027\n",
      "9th link\n",
      "0.17535663543600474\n",
      "10th link\n",
      "0.2605980091906449\n",
      "11th link\n",
      "0.2677752044830093\n",
      "12th link\n",
      "0.26809432196457095\n",
      "13th link\n",
      "0.21822644966872412\n",
      "14th link\n",
      "0.17335147764714917\n",
      "15th link\n",
      "0.36128027921638106\n",
      "16th link\n",
      "0.2037353469657516\n",
      "17th link\n",
      "0.21665970945002286\n",
      "18th link\n",
      "0.09790202182768269\n",
      "19th link\n",
      "0.3991950659697375\n",
      "20th link\n",
      "0.14337139686771277\n",
      "21th link\n",
      "0.2969770487845934\n",
      "22th link\n",
      "0.18366100038070074\n",
      "23th link\n",
      "0.3532441137242602\n",
      "24th link\n",
      "0.11038035088579141\n",
      "25th link\n",
      "0.27525378563886527\n"
     ]
    }
   ],
   "source": [
    "#计算每条路段的误差\n",
    "mape_list = []\n",
    "for i in range(num_links):\n",
    "    a1 = mape_loss_func(model_pre[:,i,:], label_test[:,i,:])\n",
    "    mape_list.append(a1)\n",
    "    print(str(i+1)+'th link')\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mape_pd = pd.Series(mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     0.075883\n",
       "17    0.097902\n",
       "23    0.110380\n",
       "19    0.143371\n",
       "2     0.153436\n",
       "5     0.164786\n",
       "13    0.173351\n",
       "8     0.175357\n",
       "21    0.183661\n",
       "4     0.195153\n",
       "15    0.203735\n",
       "16    0.216660\n",
       "12    0.218226\n",
       "9     0.260598\n",
       "0     0.262617\n",
       "10    0.267775\n",
       "11    0.268094\n",
       "24    0.275254\n",
       "20    0.296977\n",
       "6     0.307533\n",
       "7     0.332490\n",
       "22    0.353244\n",
       "14    0.361280\n",
       "1     0.390900\n",
       "18    0.399195\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_pd.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
