{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\r\n",
    "import math\r\n",
    "import tensorflow as tf\r\n",
    "import keras \r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from keras.models import load_model,Model\r\n",
    "from keras.engine.topology import Layer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 定义融合层，将深度学习算法与历史均值算法融合\r\n",
    "class Merge_Layer(Layer):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super(Merge_Layer, self).__init__(**kwargs)\r\n",
    "\r\n",
    "    def build(self, input_shape):\r\n",
    "        self.para1 = self.add_weight(shape=(input_shape[0][1], input_shape[0][2]),\r\n",
    "                                     initializer='uniform', trainable=True,\r\n",
    "                                     name='para1')\r\n",
    "        self.para2 = self.add_weight(shape=(input_shape[1][1], input_shape[1][2]),\r\n",
    "                                     initializer='uniform', trainable=True,\r\n",
    "                                     name='para2')\r\n",
    "        super(Merge_Layer, self).build(input_shape)\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        mat1 = inputs[0]\r\n",
    "        mat2 = inputs[1]\r\n",
    "        output = mat1 * self.para1 + mat2 * self.para2\r\n",
    "        # output = mat1 * 0.1 + mat2 * 0.9\r\n",
    "        return output\r\n",
    "\r\n",
    "    def compute_output_shape(self, input_shape):\r\n",
    "        return input_shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#定义精度评价指标。为防止0值附近相对误差过大而导致的异常，定义mask层。\r\n",
    "def mape_loss_func(preds, labels):\r\n",
    "    mask = labels > 5\r\n",
    "    return np.mean(np.fabs(labels[mask]-preds[mask])/labels[mask])\r\n",
    "\r\n",
    "def smape_loss_func(preds, labels):\r\n",
    "    mask= labels > 5\r\n",
    "    return np.mean(2*np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\r\n",
    "\r\n",
    "def mae_loss_func(preds, labels):\r\n",
    "    mask= labels > 5\r\n",
    "    return np.fabs((labels[mask]-preds[mask])).mean()\r\n",
    "\r\n",
    "def eliminate_nan(b):\r\n",
    "    a = np.array(b)\r\n",
    "    c = a[~np.isnan(a)]\r\n",
    "    return c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "time3 = time.time()\r\n",
    "\r\n",
    "distance = np.asarray(pd.read_csv('target_dis.csv',header = 0, index_col= 0))\r\n",
    "near_road = np.argsort(distance)\r\n",
    "\r\n",
    "flow = np.array(pd.read_csv(r\"speed_target.csv\", header= 0, index_col= 0)) #注意header=0 or None\r\n",
    "\r\n",
    "time3 = time.time()\r\n",
    "\r\n",
    "k = 5 # 参数k为需考虑的最近路段数\r\n",
    "t_p = 31 # 参数t_p为总时间序列长度（天）\r\n",
    "t_input = 12 #参数t_input为输入时间窗(5min颗粒度)\r\n",
    "t_pre = 3 #参数t_pre为预测时间窗(5min颗粒度)\r\n",
    "num_links = 82 #参数num_links为总路段数\r\n",
    "\r\n",
    "\r\n",
    "image = []\r\n",
    "for i in range(np.shape(near_road)[0]):\r\n",
    "    road_id = []\r\n",
    "    for j in range(k):\r\n",
    "        road_id.append(near_road[i][j])\r\n",
    "    image.append(flow[road_id, :])\r\n",
    "image1 = np.reshape(image, [-1, k, len(flow[0,:])])\r\n",
    "image2 = np.transpose(image1,(1,2,0))\r\n",
    "image3 = []\r\n",
    "label = []\r\n",
    "day = []\r\n",
    "\r\n",
    "for i in range(1,t_p):\r\n",
    "    for j in range(144-t_input-t_pre):\r\n",
    "        image3.append(image2[:, i*144+j:i*144+j+t_input, :][:])\r\n",
    "        label.append(flow[:, i*144+j+t_input:i*144+j+t_input+t_pre][:])\r\n",
    "        day.append(flow[:, (i-1)*144+j+t_input:(i-1)*144+j+t_input+t_pre][:])\r\n",
    "\r\n",
    "image3 = np.asarray(image3)\r\n",
    "label = np.asarray(label)\r\n",
    "day =  np.asarray(day)\r\n",
    "\r\n",
    "print(np.shape(image3))\r\n",
    "print(np.shape(label))\r\n",
    "print(np.shape(day))\r\n",
    "\r\n",
    "#划分前80%数据为训练集，最后20%数据为测试集\r\n",
    "image_train_target = image3[:np.shape(image3)[0]*1//31]\r\n",
    "image_test_target = image3[np.shape(image3)[0]*1//31:]\r\n",
    "label_train_target = label[:np.shape(label)[0]*1//31]\r\n",
    "label_test_target = label[np.shape(label)[0]*1//31:]\r\n",
    "\r\n",
    "day_train_target = day[:np.shape(day)[0]*1//31]\r\n",
    "day_test_target = day[np.shape(day)[0]*1//31:]\r\n",
    "\r\n",
    "\r\n",
    "time4 = time.time()\r\n",
    "print('input done %g' % (time4-time3))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3870, 5, 12, 82)\n",
      "(3870, 82, 3)\n",
      "(3870, 82, 3)\n",
      "input done 0.0488718\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "time3 = time.time()\r\n",
    "\r\n",
    "distance = np.asarray(pd.read_csv('source_dis.csv',header = 0, index_col= 0))\r\n",
    "near_road = np.argsort(distance)\r\n",
    "\r\n",
    "flow = np.array(pd.read_csv(r\"speed_source.csv\", header= 0, index_col= 0)) #注意header=0 or None\r\n",
    "\r\n",
    "time3 = time.time()\r\n",
    "\r\n",
    "k = 5 # 参数k为需考虑的最近路段数\r\n",
    "t_p = 31 # 参数t_p为总时间序列长度（天）\r\n",
    "t_input = 12 #参数t_input为输入时间窗(5min颗粒度)\r\n",
    "t_pre = 3 #参数t_pre为预测时间窗(5min颗粒度)\r\n",
    "num_links = 82 #参数num_links为总路段数\r\n",
    "\r\n",
    "\r\n",
    "image = []\r\n",
    "for i in range(np.shape(near_road)[0]):\r\n",
    "    road_id = []\r\n",
    "    for j in range(k):\r\n",
    "        road_id.append(near_road[i][j])\r\n",
    "    image.append(flow[road_id, :])\r\n",
    "image1 = np.reshape(image, [-1, k, len(flow[0,:])])\r\n",
    "image2 = np.transpose(image1,(1,2,0))\r\n",
    "image3 = []\r\n",
    "label = []\r\n",
    "day = []\r\n",
    "\r\n",
    "for i in range(1,t_p):\r\n",
    "    for j in range(144-t_input-t_pre):\r\n",
    "        image3.append(image2[:, i*144+j:i*144+j+t_input, :][:])\r\n",
    "        label.append(flow[:, i*144+j+t_input:i*144+j+t_input+t_pre][:])\r\n",
    "        day.append(flow[:, (i-1)*144+j+t_input:(i-1)*144+j+t_input+t_pre][:])\r\n",
    "\r\n",
    "image3 = np.asarray(image3)\r\n",
    "label = np.asarray(label)\r\n",
    "day =  np.asarray(day)\r\n",
    "\r\n",
    "print(np.shape(image3))\r\n",
    "print(np.shape(label))\r\n",
    "print(np.shape(day))\r\n",
    "\r\n",
    "#划分前80%数据为训练集，最后20%数据为测试集\r\n",
    "image_train_source = image3[:np.shape(image3)[0]*1//31]\r\n",
    "image_test_source = image3[np.shape(image3)[0]*1//31:]\r\n",
    "label_train_source = label[:np.shape(label)[0]*1//31]\r\n",
    "label_test_source = label[np.shape(label)[0]*1//31:]\r\n",
    "\r\n",
    "day_train_source = day[:np.shape(day)[0]*1//31]\r\n",
    "day_test_source = day[np.shape(day)[0]*1//31:]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "time4 = time.time()\r\n",
    "print('input done %g' % (time4-time3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3870, 5, 12, 82)\n",
      "(3870, 82, 3)\n",
      "(3870, 82, 3)\n",
      "input done 0.0488687\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#模型构建\r\n",
    "input_data = keras.Input(shape=(k,t_input,num_links), name='input_data')\r\n",
    "input_HA = keras.Input(shape=(num_links, t_pre), name='input_HA')\r\n",
    "\r\n",
    "x = keras.layers.BatchNormalization(input_shape =(k,t_input,num_links))(input_data)\r\n",
    "\r\n",
    "x = keras.layers.Conv2D(\r\n",
    "                           filters = 30,\r\n",
    "                           kernel_size = 3,\r\n",
    "                           strides = 1,\r\n",
    "                           padding=\"SAME\",\r\n",
    "                           activation='relu')(x)\r\n",
    "\r\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\r\n",
    "                                strides = 1,\r\n",
    "                                padding = \"SAME\",\r\n",
    "                                )(x)\r\n",
    "\r\n",
    "x = keras.layers.BatchNormalization()(x)\r\n",
    "\r\n",
    "x = keras.layers.Conv2D(\r\n",
    "                       filters = 30,\r\n",
    "                       kernel_size = 3,\r\n",
    "                       strides = 1,\r\n",
    "                       padding=\"SAME\",\r\n",
    "                       activation='relu')(x)\r\n",
    "\r\n",
    "x = keras.layers.AveragePooling2D(pool_size = (2,2),\r\n",
    "                                strides = 1,\r\n",
    "                                padding = \"SAME\",\r\n",
    "                                )(x)\r\n",
    "x = keras.layers.Flatten()(x)\r\n",
    "x = keras.layers.BatchNormalization()(x)\r\n",
    "x = keras.layers.Dropout(0.5)(x)\r\n",
    "x = keras.layers.Dense(num_links*t_pre, activation='relu')(x)\r\n",
    "\r\n",
    "output = keras.layers.Reshape((num_links,t_pre))(x)\r\n",
    "\r\n",
    "output_final = Merge_Layer()([output, input_HA])\r\n",
    "\r\n",
    "# construct model\r\n",
    "finish_model = keras.models.Model([input_data,input_HA], [output_final])\r\n",
    "\r\n",
    "finish_model.summary()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 5, 12, 82)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 12, 82)    328         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 12, 30)    22170       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 12, 30)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 12, 30)    120         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 12, 30)    8130        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 5, 12, 30)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1800)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1800)         7200        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1800)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 246)          443046      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 82, 3)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_HA (InputLayer)           (None, 82, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge__layer_1 (Merge_Layer)    (None, 82, 3)        492         reshape_1[0][0]                  \n",
      "                                                                 input_HA[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 481,486\n",
      "Trainable params: 477,662\n",
      "Non-trainable params: 3,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#参数加载\r\n",
    "finish_model.load_weights('source1.h5')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#模型预测\r\n",
    "model_pre = finish_model.predict([image_test_target,day_test_target])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#预测结果存储\r\n",
    "# model_pre = np.reshape(model_pre,[103, 6])\r\n",
    "# model_pre1 = pd.DataFrame(model_pre)\r\n",
    "# model_pre1.to_csv('预测值.csv', index = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#transfer without FT 预测精度计算\r\n",
    "\r\n",
    "mape_mean = mape_loss_func(model_pre, label_test_target)\r\n",
    "smape_mean = smape_loss_func(model_pre, label_test_target)\r\n",
    "mae_mean = mae_loss_func(model_pre, label_test_target)\r\n",
    "\r\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mape = 0.8883346348539743\n",
      "smape = 1.1060631285712377\n",
      "mae = 50.212213331057384\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from keras import backend as K\r\n",
    "from sklearn import metrics\r\n",
    "def mmd (x, y):\r\n",
    "    return metrics.mean_squared_error(x,y)\r\n",
    "\r\n",
    "def new_loss(output_final, label_train_target):\r\n",
    "    middle = Model(inputs=[input_data, input_HA],outputs=finish_model.get_layer('dense_1').output)\r\n",
    "    middle_result_source = middle.predict([image_train_source, day_train_source])\r\n",
    "    middle_result_target = middle.predict([image_train_target, day_train_target])\r\n",
    "\r\n",
    "    loss1 = K.mean(K.square(output_final - label_train_target), axis=-1) \r\n",
    "    loss2 = 0.01 * mmd (middle_result_source, middle_result_target)\r\n",
    "    overall_loss = loss1 + loss2\r\n",
    "    return overall_loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "finish_model.compile(optimizer='adam',loss=new_loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ad4\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "finish_model.fit([image_train_target, day_train_target], label_train_target, epochs=600, batch_size=512,\r\n",
    "validation_data=([image_test_target,day_test_target], label_test_target), verbose = 2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 124 samples, validate on 3746 samples\n",
      "Epoch 1/600\n",
      " - 1s - loss: 1147.8462 - val_loss: 1500.1749\n",
      "Epoch 2/600\n",
      " - 0s - loss: 1045.4607 - val_loss: 1461.1951\n",
      "Epoch 3/600\n",
      " - 0s - loss: 1018.6842 - val_loss: 1449.2982\n",
      "Epoch 4/600\n",
      " - 0s - loss: 995.3873 - val_loss: 1448.3197\n",
      "Epoch 5/600\n",
      " - 0s - loss: 971.0936 - val_loss: 1438.2459\n",
      "Epoch 6/600\n",
      " - 0s - loss: 955.2532 - val_loss: 1426.5414\n",
      "Epoch 7/600\n",
      " - 0s - loss: 943.3039 - val_loss: 1419.5918\n",
      "Epoch 8/600\n",
      " - 0s - loss: 934.0153 - val_loss: 1414.5475\n",
      "Epoch 9/600\n",
      " - 0s - loss: 925.4224 - val_loss: 1414.3330\n",
      "Epoch 10/600\n",
      " - 0s - loss: 914.7467 - val_loss: 1418.8998\n",
      "Epoch 11/600\n",
      " - 0s - loss: 905.8820 - val_loss: 1418.5711\n",
      "Epoch 12/600\n",
      " - 0s - loss: 895.9669 - val_loss: 1401.5500\n",
      "Epoch 13/600\n",
      " - 0s - loss: 892.4910 - val_loss: 1374.4628\n",
      "Epoch 14/600\n",
      " - 0s - loss: 883.0661 - val_loss: 1339.0083\n",
      "Epoch 15/600\n",
      " - 0s - loss: 875.9177 - val_loss: 1309.5152\n",
      "Epoch 16/600\n",
      " - 0s - loss: 870.0261 - val_loss: 1288.4122\n",
      "Epoch 17/600\n",
      " - 0s - loss: 864.0709 - val_loss: 1270.1483\n",
      "Epoch 18/600\n",
      " - 0s - loss: 859.6282 - val_loss: 1256.1894\n",
      "Epoch 19/600\n",
      " - 0s - loss: 856.0829 - val_loss: 1245.1650\n",
      "Epoch 20/600\n",
      " - 0s - loss: 849.8842 - val_loss: 1233.5421\n",
      "Epoch 21/600\n",
      " - 0s - loss: 844.2711 - val_loss: 1221.2074\n",
      "Epoch 22/600\n",
      " - 0s - loss: 840.7211 - val_loss: 1207.2039\n",
      "Epoch 23/600\n",
      " - 0s - loss: 836.5813 - val_loss: 1191.3156\n",
      "Epoch 24/600\n",
      " - 0s - loss: 833.1165 - val_loss: 1176.3003\n",
      "Epoch 25/600\n",
      " - 0s - loss: 830.6419 - val_loss: 1158.8006\n",
      "Epoch 26/600\n",
      " - 0s - loss: 827.1840 - val_loss: 1144.7342\n",
      "Epoch 27/600\n",
      " - 0s - loss: 825.9875 - val_loss: 1132.5890\n",
      "Epoch 28/600\n",
      " - 0s - loss: 819.9356 - val_loss: 1123.0168\n",
      "Epoch 29/600\n",
      " - 0s - loss: 818.6422 - val_loss: 1114.5751\n",
      "Epoch 30/600\n",
      " - 0s - loss: 813.5270 - val_loss: 1105.9196\n",
      "Epoch 31/600\n",
      " - 0s - loss: 812.9766 - val_loss: 1098.3473\n",
      "Epoch 32/600\n",
      " - 0s - loss: 809.3933 - val_loss: 1089.0492\n",
      "Epoch 33/600\n",
      " - 0s - loss: 807.9518 - val_loss: 1080.0233\n",
      "Epoch 34/600\n",
      " - 0s - loss: 804.9501 - val_loss: 1071.5251\n",
      "Epoch 35/600\n",
      " - 0s - loss: 803.2308 - val_loss: 1062.3233\n",
      "Epoch 36/600\n",
      " - 0s - loss: 802.3888 - val_loss: 1053.7254\n",
      "Epoch 37/600\n",
      " - 0s - loss: 799.2193 - val_loss: 1047.3499\n",
      "Epoch 38/600\n",
      " - 0s - loss: 799.7488 - val_loss: 1043.5245\n",
      "Epoch 39/600\n",
      " - 0s - loss: 797.0471 - val_loss: 1041.7779\n",
      "Epoch 40/600\n",
      " - 0s - loss: 796.6514 - val_loss: 1037.9215\n",
      "Epoch 41/600\n",
      " - 0s - loss: 794.8639 - val_loss: 1032.1112\n",
      "Epoch 42/600\n",
      " - 0s - loss: 791.7188 - val_loss: 1027.2934\n",
      "Epoch 43/600\n",
      " - 0s - loss: 790.9806 - val_loss: 1023.0714\n",
      "Epoch 44/600\n",
      " - 0s - loss: 790.1522 - val_loss: 1020.3443\n",
      "Epoch 45/600\n",
      " - 0s - loss: 789.4919 - val_loss: 1017.0695\n",
      "Epoch 46/600\n",
      " - 0s - loss: 787.8763 - val_loss: 1013.3519\n",
      "Epoch 47/600\n",
      " - 0s - loss: 788.4369 - val_loss: 1012.3119\n",
      "Epoch 48/600\n",
      " - 0s - loss: 787.1991 - val_loss: 1010.2871\n",
      "Epoch 49/600\n",
      " - 0s - loss: 785.5729 - val_loss: 1006.8472\n",
      "Epoch 50/600\n",
      " - 0s - loss: 787.3853 - val_loss: 1005.0437\n",
      "Epoch 51/600\n",
      " - 0s - loss: 784.3653 - val_loss: 1004.0781\n",
      "Epoch 52/600\n",
      " - 0s - loss: 784.3145 - val_loss: 1004.1415\n",
      "Epoch 53/600\n",
      " - 0s - loss: 782.5182 - val_loss: 1005.9800\n",
      "Epoch 54/600\n",
      " - 0s - loss: 782.8179 - val_loss: 1008.1869\n",
      "Epoch 55/600\n",
      " - 0s - loss: 783.0529 - val_loss: 1010.0338\n",
      "Epoch 56/600\n",
      " - 0s - loss: 780.7478 - val_loss: 1011.3426\n",
      "Epoch 57/600\n",
      " - 0s - loss: 780.8440 - val_loss: 1011.9056\n",
      "Epoch 58/600\n",
      " - 0s - loss: 779.1004 - val_loss: 1011.3757\n",
      "Epoch 59/600\n",
      " - 0s - loss: 779.1102 - val_loss: 1007.4634\n",
      "Epoch 60/600\n",
      " - 0s - loss: 776.4974 - val_loss: 1001.2168\n",
      "Epoch 61/600\n",
      " - 0s - loss: 777.4525 - val_loss: 993.7429\n",
      "Epoch 62/600\n",
      " - 0s - loss: 777.9407 - val_loss: 986.1062\n",
      "Epoch 63/600\n",
      " - 0s - loss: 776.5067 - val_loss: 981.0534\n",
      "Epoch 64/600\n",
      " - 0s - loss: 777.9813 - val_loss: 977.2054\n",
      "Epoch 65/600\n",
      " - 0s - loss: 775.9642 - val_loss: 976.2413\n",
      "Epoch 66/600\n",
      " - 0s - loss: 774.9528 - val_loss: 975.8711\n",
      "Epoch 67/600\n",
      " - 0s - loss: 775.2180 - val_loss: 977.2468\n",
      "Epoch 68/600\n",
      " - 0s - loss: 774.6542 - val_loss: 978.9278\n",
      "Epoch 69/600\n",
      " - 0s - loss: 775.8218 - val_loss: 981.8300\n",
      "Epoch 70/600\n",
      " - 0s - loss: 773.9958 - val_loss: 984.5719\n",
      "Epoch 71/600\n",
      " - 0s - loss: 775.2861 - val_loss: 986.0785\n",
      "Epoch 72/600\n",
      " - 0s - loss: 773.8395 - val_loss: 986.3494\n",
      "Epoch 73/600\n",
      " - 0s - loss: 773.5162 - val_loss: 985.2249\n",
      "Epoch 74/600\n",
      " - 0s - loss: 775.1218 - val_loss: 983.1995\n",
      "Epoch 75/600\n",
      " - 0s - loss: 772.3168 - val_loss: 979.4867\n",
      "Epoch 76/600\n",
      " - 0s - loss: 771.6662 - val_loss: 974.4002\n",
      "Epoch 77/600\n",
      " - 0s - loss: 771.9874 - val_loss: 971.3037\n",
      "Epoch 78/600\n",
      " - 0s - loss: 771.4381 - val_loss: 968.7760\n",
      "Epoch 79/600\n",
      " - 0s - loss: 770.7277 - val_loss: 968.9500\n",
      "Epoch 80/600\n",
      " - 0s - loss: 771.7654 - val_loss: 969.7228\n",
      "Epoch 81/600\n",
      " - 0s - loss: 771.8235 - val_loss: 971.9264\n",
      "Epoch 82/600\n",
      " - 0s - loss: 770.1784 - val_loss: 974.6280\n",
      "Epoch 83/600\n",
      " - 0s - loss: 767.9373 - val_loss: 976.5372\n",
      "Epoch 84/600\n",
      " - 0s - loss: 769.7747 - val_loss: 978.2009\n",
      "Epoch 85/600\n",
      " - 0s - loss: 768.9156 - val_loss: 978.5626\n",
      "Epoch 86/600\n",
      " - 0s - loss: 769.2075 - val_loss: 977.6986\n",
      "Epoch 87/600\n",
      " - 0s - loss: 769.8193 - val_loss: 974.1913\n",
      "Epoch 88/600\n",
      " - 0s - loss: 769.6031 - val_loss: 969.1885\n",
      "Epoch 89/600\n",
      " - 0s - loss: 769.6375 - val_loss: 964.1471\n",
      "Epoch 90/600\n",
      " - 0s - loss: 768.6248 - val_loss: 960.0102\n",
      "Epoch 91/600\n",
      " - 0s - loss: 768.5536 - val_loss: 956.7414\n",
      "Epoch 92/600\n",
      " - 0s - loss: 770.0704 - val_loss: 955.6359\n",
      "Epoch 93/600\n",
      " - 0s - loss: 767.8557 - val_loss: 956.0465\n",
      "Epoch 94/600\n",
      " - 0s - loss: 769.5376 - val_loss: 957.4576\n",
      "Epoch 95/600\n",
      " - 0s - loss: 767.0244 - val_loss: 958.1343\n",
      "Epoch 96/600\n",
      " - 0s - loss: 769.3464 - val_loss: 959.1847\n",
      "Epoch 97/600\n",
      " - 0s - loss: 767.3668 - val_loss: 957.9674\n",
      "Epoch 98/600\n",
      " - 0s - loss: 766.4061 - val_loss: 956.2121\n",
      "Epoch 99/600\n",
      " - 0s - loss: 765.5026 - val_loss: 956.1872\n",
      "Epoch 100/600\n",
      " - 0s - loss: 768.1176 - val_loss: 956.8345\n",
      "Epoch 101/600\n",
      " - 0s - loss: 767.5872 - val_loss: 955.3730\n",
      "Epoch 102/600\n",
      " - 0s - loss: 765.6156 - val_loss: 953.5912\n",
      "Epoch 103/600\n",
      " - 0s - loss: 766.3417 - val_loss: 952.7771\n",
      "Epoch 104/600\n",
      " - 0s - loss: 765.2521 - val_loss: 952.3496\n",
      "Epoch 105/600\n",
      " - 0s - loss: 764.7546 - val_loss: 954.0330\n",
      "Epoch 106/600\n",
      " - 0s - loss: 765.7199 - val_loss: 955.5559\n",
      "Epoch 107/600\n",
      " - 0s - loss: 764.4576 - val_loss: 957.7790\n",
      "Epoch 108/600\n",
      " - 0s - loss: 763.8401 - val_loss: 959.1004\n",
      "Epoch 109/600\n",
      " - 0s - loss: 764.4337 - val_loss: 960.3453\n",
      "Epoch 110/600\n",
      " - 0s - loss: 763.9690 - val_loss: 961.3525\n",
      "Epoch 111/600\n",
      " - 0s - loss: 764.1269 - val_loss: 962.0769\n",
      "Epoch 112/600\n",
      " - 0s - loss: 764.2272 - val_loss: 962.5298\n",
      "Epoch 113/600\n",
      " - 0s - loss: 762.9030 - val_loss: 959.9528\n",
      "Epoch 114/600\n",
      " - 0s - loss: 763.3583 - val_loss: 957.6948\n",
      "Epoch 115/600\n",
      " - 0s - loss: 763.9857 - val_loss: 954.2969\n",
      "Epoch 116/600\n",
      " - 0s - loss: 763.4044 - val_loss: 952.2358\n",
      "Epoch 117/600\n",
      " - 0s - loss: 763.4452 - val_loss: 951.0255\n",
      "Epoch 118/600\n",
      " - 0s - loss: 764.3344 - val_loss: 952.1829\n",
      "Epoch 119/600\n",
      " - 0s - loss: 762.5125 - val_loss: 954.2686\n",
      "Epoch 120/600\n",
      " - 0s - loss: 764.4565 - val_loss: 957.2192\n",
      "Epoch 121/600\n",
      " - 0s - loss: 762.5015 - val_loss: 958.5968\n",
      "Epoch 122/600\n",
      " - 0s - loss: 763.4849 - val_loss: 959.1310\n",
      "Epoch 123/600\n",
      " - 0s - loss: 764.5759 - val_loss: 958.3198\n",
      "Epoch 124/600\n",
      " - 0s - loss: 762.5240 - val_loss: 954.8235\n",
      "Epoch 125/600\n",
      " - 0s - loss: 762.3053 - val_loss: 950.8252\n",
      "Epoch 126/600\n",
      " - 0s - loss: 760.6620 - val_loss: 947.5847\n",
      "Epoch 127/600\n",
      " - 0s - loss: 761.6931 - val_loss: 946.5336\n",
      "Epoch 128/600\n",
      " - 0s - loss: 763.1973 - val_loss: 947.8805\n",
      "Epoch 129/600\n",
      " - 0s - loss: 761.6536 - val_loss: 950.7400\n",
      "Epoch 130/600\n",
      " - 0s - loss: 761.3365 - val_loss: 951.9221\n",
      "Epoch 131/600\n",
      " - 0s - loss: 760.0706 - val_loss: 952.7991\n",
      "Epoch 132/600\n",
      " - 0s - loss: 762.9477 - val_loss: 952.3995\n",
      "Epoch 133/600\n",
      " - 0s - loss: 761.5966 - val_loss: 951.9830\n",
      "Epoch 134/600\n",
      " - 0s - loss: 762.6394 - val_loss: 951.3494\n",
      "Epoch 135/600\n",
      " - 0s - loss: 760.4821 - val_loss: 952.4830\n",
      "Epoch 136/600\n",
      " - 0s - loss: 762.2162 - val_loss: 954.0924\n",
      "Epoch 137/600\n",
      " - 0s - loss: 760.8768 - val_loss: 953.5404\n",
      "Epoch 138/600\n",
      " - 0s - loss: 760.2192 - val_loss: 952.7175\n",
      "Epoch 139/600\n",
      " - 0s - loss: 760.2061 - val_loss: 951.4950\n",
      "Epoch 140/600\n",
      " - 0s - loss: 760.8705 - val_loss: 949.2832\n",
      "Epoch 141/600\n",
      " - 0s - loss: 760.8158 - val_loss: 947.3358\n",
      "Epoch 142/600\n",
      " - 0s - loss: 759.2980 - val_loss: 946.6818\n",
      "Epoch 143/600\n",
      " - 0s - loss: 760.5422 - val_loss: 946.0182\n",
      "Epoch 144/600\n",
      " - 0s - loss: 759.9235 - val_loss: 946.7329\n",
      "Epoch 145/600\n",
      " - 0s - loss: 759.9284 - val_loss: 947.4930\n",
      "Epoch 146/600\n",
      " - 0s - loss: 759.3549 - val_loss: 947.3215\n",
      "Epoch 147/600\n",
      " - 0s - loss: 760.7112 - val_loss: 947.1063\n",
      "Epoch 148/600\n",
      " - 0s - loss: 760.8517 - val_loss: 947.0023\n",
      "Epoch 149/600\n",
      " - 0s - loss: 758.0598 - val_loss: 946.9904\n",
      "Epoch 150/600\n",
      " - 0s - loss: 759.9957 - val_loss: 944.8826\n",
      "Epoch 151/600\n",
      " - 0s - loss: 759.9760 - val_loss: 941.9286\n",
      "Epoch 152/600\n",
      " - 0s - loss: 759.7830 - val_loss: 937.1129\n",
      "Epoch 153/600\n",
      " - 0s - loss: 760.9845 - val_loss: 933.5744\n",
      "Epoch 154/600\n",
      " - 0s - loss: 760.4180 - val_loss: 929.9614\n",
      "Epoch 155/600\n",
      " - 0s - loss: 760.9564 - val_loss: 927.7647\n",
      "Epoch 156/600\n",
      " - 0s - loss: 759.5170 - val_loss: 926.8075\n",
      "Epoch 157/600\n",
      " - 0s - loss: 758.8819 - val_loss: 926.7779\n",
      "Epoch 158/600\n",
      " - 0s - loss: 761.5568 - val_loss: 928.7251\n",
      "Epoch 159/600\n",
      " - 0s - loss: 758.1510 - val_loss: 931.2891\n",
      "Epoch 160/600\n",
      " - 0s - loss: 759.1351 - val_loss: 932.2854\n",
      "Epoch 161/600\n",
      " - 0s - loss: 759.0591 - val_loss: 932.5015\n",
      "Epoch 162/600\n",
      " - 0s - loss: 759.5930 - val_loss: 935.1719\n",
      "Epoch 163/600\n",
      " - 0s - loss: 759.1638 - val_loss: 935.7181\n",
      "Epoch 164/600\n",
      " - 0s - loss: 758.5819 - val_loss: 936.0111\n",
      "Epoch 165/600\n",
      " - 0s - loss: 758.6550 - val_loss: 934.9572\n",
      "Epoch 166/600\n",
      " - 0s - loss: 757.9630 - val_loss: 933.5408\n",
      "Epoch 167/600\n",
      " - 0s - loss: 757.7964 - val_loss: 933.4594\n",
      "Epoch 168/600\n",
      " - 0s - loss: 756.4816 - val_loss: 934.6727\n",
      "Epoch 169/600\n",
      " - 0s - loss: 758.8997 - val_loss: 935.1786\n",
      "Epoch 170/600\n",
      " - 0s - loss: 758.5056 - val_loss: 935.9319\n",
      "Epoch 171/600\n",
      " - 0s - loss: 756.2530 - val_loss: 936.2408\n",
      "Epoch 172/600\n",
      " - 0s - loss: 758.3665 - val_loss: 935.4621\n",
      "Epoch 173/600\n",
      " - 0s - loss: 758.0570 - val_loss: 934.9313\n",
      "Epoch 174/600\n",
      " - 0s - loss: 757.3486 - val_loss: 934.4316\n",
      "Epoch 175/600\n",
      " - 0s - loss: 757.2054 - val_loss: 933.8968\n",
      "Epoch 176/600\n",
      " - 0s - loss: 758.1597 - val_loss: 932.7602\n",
      "Epoch 177/600\n",
      " - 0s - loss: 756.0256 - val_loss: 932.2540\n",
      "Epoch 178/600\n",
      " - 0s - loss: 756.7302 - val_loss: 931.9231\n",
      "Epoch 179/600\n",
      " - 0s - loss: 756.8297 - val_loss: 931.3963\n",
      "Epoch 180/600\n",
      " - 0s - loss: 757.4303 - val_loss: 931.5688\n",
      "Epoch 181/600\n",
      " - 0s - loss: 757.2618 - val_loss: 930.7386\n",
      "Epoch 182/600\n",
      " - 0s - loss: 757.7790 - val_loss: 930.1865\n",
      "Epoch 183/600\n",
      " - 0s - loss: 757.0167 - val_loss: 929.5430\n",
      "Epoch 184/600\n",
      " - 0s - loss: 757.8890 - val_loss: 930.1727\n",
      "Epoch 185/600\n",
      " - 0s - loss: 756.9061 - val_loss: 932.0557\n",
      "Epoch 186/600\n",
      " - 0s - loss: 757.7655 - val_loss: 930.8324\n",
      "Epoch 187/600\n",
      " - 0s - loss: 756.5125 - val_loss: 926.9586\n",
      "Epoch 188/600\n",
      " - 0s - loss: 757.4398 - val_loss: 922.5233\n",
      "Epoch 189/600\n",
      " - 0s - loss: 756.6996 - val_loss: 919.5988\n",
      "Epoch 190/600\n",
      " - 0s - loss: 757.6104 - val_loss: 918.9129\n",
      "Epoch 191/600\n",
      " - 0s - loss: 757.5165 - val_loss: 918.3104\n",
      "Epoch 192/600\n",
      " - 0s - loss: 757.6587 - val_loss: 917.4370\n",
      "Epoch 193/600\n",
      " - 0s - loss: 755.3837 - val_loss: 918.7578\n",
      "Epoch 194/600\n",
      " - 0s - loss: 755.2718 - val_loss: 919.2694\n",
      "Epoch 195/600\n",
      " - 0s - loss: 754.2656 - val_loss: 919.6234\n",
      "Epoch 196/600\n",
      " - 0s - loss: 756.3024 - val_loss: 919.5265\n",
      "Epoch 197/600\n",
      " - 0s - loss: 756.2496 - val_loss: 919.8684\n",
      "Epoch 198/600\n",
      " - 0s - loss: 755.8655 - val_loss: 920.1910\n",
      "Epoch 199/600\n",
      " - 0s - loss: 755.8508 - val_loss: 919.8615\n",
      "Epoch 200/600\n",
      " - 0s - loss: 757.2258 - val_loss: 919.1361\n",
      "Epoch 201/600\n",
      " - 0s - loss: 755.6410 - val_loss: 918.2908\n",
      "Epoch 202/600\n",
      " - 0s - loss: 755.6890 - val_loss: 916.2274\n",
      "Epoch 203/600\n",
      " - 0s - loss: 756.8569 - val_loss: 914.3016\n",
      "Epoch 204/600\n",
      " - 0s - loss: 757.6926 - val_loss: 913.2534\n",
      "Epoch 205/600\n",
      " - 0s - loss: 757.2490 - val_loss: 914.4194\n",
      "Epoch 206/600\n",
      " - 0s - loss: 754.9376 - val_loss: 915.7643\n",
      "Epoch 207/600\n",
      " - 0s - loss: 754.0870 - val_loss: 916.7696\n",
      "Epoch 208/600\n",
      " - 0s - loss: 754.3891 - val_loss: 917.5313\n",
      "Epoch 209/600\n",
      " - 0s - loss: 754.9087 - val_loss: 918.1423\n",
      "Epoch 210/600\n",
      " - 0s - loss: 755.8663 - val_loss: 919.0075\n",
      "Epoch 211/600\n",
      " - 0s - loss: 755.7330 - val_loss: 919.7605\n",
      "Epoch 212/600\n",
      " - 0s - loss: 754.8591 - val_loss: 920.2835\n",
      "Epoch 213/600\n",
      " - 0s - loss: 754.2652 - val_loss: 920.3767\n",
      "Epoch 214/600\n",
      " - 0s - loss: 753.6543 - val_loss: 919.1844\n",
      "Epoch 215/600\n",
      " - 0s - loss: 754.3495 - val_loss: 917.8475\n",
      "Epoch 216/600\n",
      " - 0s - loss: 755.6074 - val_loss: 916.4472\n",
      "Epoch 217/600\n",
      " - 0s - loss: 753.5794 - val_loss: 914.6808\n",
      "Epoch 218/600\n",
      " - 0s - loss: 754.8105 - val_loss: 912.0489\n",
      "Epoch 219/600\n",
      " - 0s - loss: 755.1902 - val_loss: 910.1665\n",
      "Epoch 220/600\n",
      " - 0s - loss: 753.4633 - val_loss: 909.2730\n",
      "Epoch 221/600\n",
      " - 0s - loss: 754.4185 - val_loss: 909.6097\n",
      "Epoch 222/600\n",
      " - 0s - loss: 755.6228 - val_loss: 913.1595\n",
      "Epoch 223/600\n",
      " - 0s - loss: 754.4091 - val_loss: 916.1008\n",
      "Epoch 224/600\n",
      " - 0s - loss: 756.1484 - val_loss: 918.5023\n",
      "Epoch 225/600\n",
      " - 0s - loss: 754.3116 - val_loss: 919.5822\n",
      "Epoch 226/600\n",
      " - 0s - loss: 755.2142 - val_loss: 918.4921\n",
      "Epoch 227/600\n",
      " - 0s - loss: 754.5098 - val_loss: 916.7599\n",
      "Epoch 228/600\n",
      " - 0s - loss: 753.7630 - val_loss: 914.1104\n",
      "Epoch 229/600\n",
      " - 0s - loss: 753.7079 - val_loss: 912.5131\n",
      "Epoch 230/600\n",
      " - 0s - loss: 753.7425 - val_loss: 911.1242\n",
      "Epoch 231/600\n",
      " - 0s - loss: 754.9277 - val_loss: 910.5981\n",
      "Epoch 232/600\n",
      " - 0s - loss: 755.3554 - val_loss: 910.6597\n",
      "Epoch 233/600\n",
      " - 0s - loss: 753.5577 - val_loss: 911.0140\n",
      "Epoch 234/600\n",
      " - 0s - loss: 753.3589 - val_loss: 912.8861\n",
      "Epoch 235/600\n",
      " - 0s - loss: 753.5116 - val_loss: 916.4387\n",
      "Epoch 236/600\n",
      " - 0s - loss: 753.8981 - val_loss: 918.9834\n",
      "Epoch 237/600\n",
      " - 0s - loss: 753.9194 - val_loss: 920.1686\n",
      "Epoch 238/600\n",
      " - 0s - loss: 754.2925 - val_loss: 921.6557\n",
      "Epoch 239/600\n",
      " - 0s - loss: 753.7189 - val_loss: 922.6199\n",
      "Epoch 240/600\n",
      " - 0s - loss: 754.3920 - val_loss: 922.5864\n",
      "Epoch 241/600\n",
      " - 0s - loss: 753.0977 - val_loss: 921.3866\n",
      "Epoch 242/600\n",
      " - 0s - loss: 752.7064 - val_loss: 918.8021\n",
      "Epoch 243/600\n",
      " - 0s - loss: 754.1408 - val_loss: 914.9443\n",
      "Epoch 244/600\n",
      " - 0s - loss: 754.5248 - val_loss: 912.0287\n",
      "Epoch 245/600\n",
      " - 0s - loss: 753.3593 - val_loss: 911.6939\n",
      "Epoch 246/600\n",
      " - 0s - loss: 754.2125 - val_loss: 912.3873\n",
      "Epoch 247/600\n",
      " - 0s - loss: 753.0284 - val_loss: 913.2069\n",
      "Epoch 248/600\n",
      " - 0s - loss: 753.2070 - val_loss: 913.0412\n",
      "Epoch 249/600\n",
      " - 0s - loss: 752.3433 - val_loss: 912.8035\n",
      "Epoch 250/600\n",
      " - 0s - loss: 751.9397 - val_loss: 911.2379\n",
      "Epoch 251/600\n",
      " - 0s - loss: 753.8606 - val_loss: 908.1653\n",
      "Epoch 252/600\n",
      " - 0s - loss: 752.3723 - val_loss: 906.5049\n",
      "Epoch 253/600\n",
      " - 0s - loss: 752.2778 - val_loss: 904.7924\n",
      "Epoch 254/600\n",
      " - 0s - loss: 753.2512 - val_loss: 902.3493\n",
      "Epoch 255/600\n",
      " - 0s - loss: 752.7394 - val_loss: 901.5404\n",
      "Epoch 256/600\n",
      " - 0s - loss: 751.8316 - val_loss: 901.7810\n",
      "Epoch 257/600\n",
      " - 0s - loss: 752.4264 - val_loss: 902.1939\n",
      "Epoch 258/600\n",
      " - 0s - loss: 753.1265 - val_loss: 902.5370\n",
      "Epoch 259/600\n",
      " - 0s - loss: 752.7465 - val_loss: 904.4688\n",
      "Epoch 260/600\n",
      " - 0s - loss: 752.7211 - val_loss: 907.4404\n",
      "Epoch 261/600\n",
      " - 0s - loss: 751.4108 - val_loss: 909.3190\n",
      "Epoch 262/600\n",
      " - 0s - loss: 753.7099 - val_loss: 910.8325\n",
      "Epoch 263/600\n",
      " - 0s - loss: 751.7278 - val_loss: 910.1868\n",
      "Epoch 264/600\n",
      " - 0s - loss: 752.7488 - val_loss: 908.2731\n",
      "Epoch 265/600\n",
      " - 0s - loss: 752.2255 - val_loss: 905.8784\n",
      "Epoch 266/600\n",
      " - 0s - loss: 752.0502 - val_loss: 903.4211\n",
      "Epoch 267/600\n",
      " - 0s - loss: 752.0266 - val_loss: 902.7513\n",
      "Epoch 268/600\n",
      " - 0s - loss: 751.3226 - val_loss: 903.2794\n",
      "Epoch 269/600\n",
      " - 0s - loss: 751.3458 - val_loss: 904.8101\n",
      "Epoch 270/600\n",
      " - 0s - loss: 751.2634 - val_loss: 905.6532\n",
      "Epoch 271/600\n",
      " - 0s - loss: 752.3028 - val_loss: 907.3252\n",
      "Epoch 272/600\n",
      " - 0s - loss: 752.3094 - val_loss: 908.2933\n",
      "Epoch 273/600\n",
      " - 0s - loss: 751.4841 - val_loss: 908.2821\n",
      "Epoch 274/600\n",
      " - 0s - loss: 751.9636 - val_loss: 907.8606\n",
      "Epoch 275/600\n",
      " - 0s - loss: 753.0414 - val_loss: 907.0523\n",
      "Epoch 276/600\n",
      " - 0s - loss: 751.8051 - val_loss: 905.3755\n",
      "Epoch 277/600\n",
      " - 0s - loss: 750.8297 - val_loss: 903.9079\n",
      "Epoch 278/600\n",
      " - 0s - loss: 752.8047 - val_loss: 904.1072\n",
      "Epoch 279/600\n",
      " - 0s - loss: 751.9283 - val_loss: 904.1167\n",
      "Epoch 280/600\n",
      " - 0s - loss: 751.6585 - val_loss: 905.0629\n",
      "Epoch 281/600\n",
      " - 0s - loss: 751.9048 - val_loss: 905.9371\n",
      "Epoch 282/600\n",
      " - 0s - loss: 753.5461 - val_loss: 908.1441\n",
      "Epoch 283/600\n",
      " - 0s - loss: 751.8808 - val_loss: 910.4508\n",
      "Epoch 284/600\n",
      " - 0s - loss: 751.6365 - val_loss: 911.7109\n",
      "Epoch 285/600\n",
      " - 0s - loss: 750.6885 - val_loss: 912.3359\n",
      "Epoch 286/600\n",
      " - 0s - loss: 751.1606 - val_loss: 911.5217\n",
      "Epoch 287/600\n",
      " - 0s - loss: 750.8965 - val_loss: 910.1812\n",
      "Epoch 288/600\n",
      " - 0s - loss: 751.7924 - val_loss: 910.0460\n",
      "Epoch 289/600\n",
      " - 0s - loss: 751.8180 - val_loss: 910.6449\n",
      "Epoch 290/600\n",
      " - 0s - loss: 752.1908 - val_loss: 911.1896\n",
      "Epoch 291/600\n",
      " - 0s - loss: 752.3225 - val_loss: 910.2530\n",
      "Epoch 292/600\n",
      " - 0s - loss: 750.5504 - val_loss: 909.9295\n",
      "Epoch 293/600\n",
      " - 0s - loss: 751.5898 - val_loss: 910.6534\n",
      "Epoch 294/600\n",
      " - 0s - loss: 750.2562 - val_loss: 910.2378\n",
      "Epoch 295/600\n",
      " - 0s - loss: 751.4256 - val_loss: 908.0553\n",
      "Epoch 296/600\n",
      " - 0s - loss: 752.1329 - val_loss: 905.4183\n",
      "Epoch 297/600\n",
      " - 0s - loss: 752.4628 - val_loss: 902.3164\n",
      "Epoch 298/600\n",
      " - 0s - loss: 752.4003 - val_loss: 901.5311\n",
      "Epoch 299/600\n",
      " - 0s - loss: 751.4721 - val_loss: 902.7566\n",
      "Epoch 300/600\n",
      " - 0s - loss: 750.2548 - val_loss: 903.9731\n",
      "Epoch 301/600\n",
      " - 0s - loss: 751.6065 - val_loss: 904.3861\n",
      "Epoch 302/600\n",
      " - 0s - loss: 751.2927 - val_loss: 904.4946\n",
      "Epoch 303/600\n",
      " - 0s - loss: 750.9253 - val_loss: 904.8787\n",
      "Epoch 304/600\n",
      " - 0s - loss: 752.4412 - val_loss: 904.8976\n",
      "Epoch 305/600\n",
      " - 0s - loss: 750.1879 - val_loss: 904.4707\n",
      "Epoch 306/600\n",
      " - 0s - loss: 750.4385 - val_loss: 903.9761\n",
      "Epoch 307/600\n",
      " - 0s - loss: 751.8137 - val_loss: 903.7653\n",
      "Epoch 308/600\n",
      " - 0s - loss: 748.8313 - val_loss: 902.9040\n",
      "Epoch 309/600\n",
      " - 0s - loss: 752.0873 - val_loss: 902.5112\n",
      "Epoch 310/600\n",
      " - 0s - loss: 749.9487 - val_loss: 901.7835\n",
      "Epoch 311/600\n",
      " - 0s - loss: 751.1203 - val_loss: 900.6236\n",
      "Epoch 312/600\n",
      " - 0s - loss: 751.0892 - val_loss: 898.8716\n",
      "Epoch 313/600\n",
      " - 0s - loss: 751.8030 - val_loss: 899.5738\n",
      "Epoch 314/600\n",
      " - 0s - loss: 752.1500 - val_loss: 900.5765\n",
      "Epoch 315/600\n",
      " - 0s - loss: 750.8878 - val_loss: 902.3682\n",
      "Epoch 316/600\n",
      " - 0s - loss: 749.9899 - val_loss: 903.1843\n",
      "Epoch 317/600\n",
      " - 0s - loss: 751.3361 - val_loss: 904.4319\n",
      "Epoch 318/600\n",
      " - 0s - loss: 750.7921 - val_loss: 905.5763\n",
      "Epoch 319/600\n",
      " - 0s - loss: 749.5410 - val_loss: 905.6083\n",
      "Epoch 320/600\n",
      " - 0s - loss: 749.8813 - val_loss: 903.8556\n",
      "Epoch 321/600\n",
      " - 0s - loss: 749.2425 - val_loss: 902.7155\n",
      "Epoch 322/600\n",
      " - 0s - loss: 749.2471 - val_loss: 901.1766\n",
      "Epoch 323/600\n",
      " - 0s - loss: 750.1769 - val_loss: 899.4134\n",
      "Epoch 324/600\n",
      " - 0s - loss: 750.2538 - val_loss: 897.8963\n",
      "Epoch 325/600\n",
      " - 0s - loss: 750.3463 - val_loss: 897.3061\n",
      "Epoch 326/600\n",
      " - 0s - loss: 750.7252 - val_loss: 898.0291\n",
      "Epoch 327/600\n",
      " - 0s - loss: 749.0473 - val_loss: 900.4128\n",
      "Epoch 328/600\n",
      " - 0s - loss: 750.5524 - val_loss: 902.5058\n",
      "Epoch 329/600\n",
      " - 0s - loss: 751.3989 - val_loss: 904.1868\n",
      "Epoch 330/600\n",
      " - 0s - loss: 749.4543 - val_loss: 905.8070\n",
      "Epoch 331/600\n",
      " - 0s - loss: 749.6384 - val_loss: 907.5958\n",
      "Epoch 332/600\n",
      " - 0s - loss: 750.6447 - val_loss: 907.9210\n",
      "Epoch 333/600\n",
      " - 0s - loss: 749.5912 - val_loss: 906.2616\n",
      "Epoch 334/600\n",
      " - 0s - loss: 750.5593 - val_loss: 903.6360\n",
      "Epoch 335/600\n",
      " - 0s - loss: 750.0864 - val_loss: 901.7940\n",
      "Epoch 336/600\n",
      " - 0s - loss: 750.4225 - val_loss: 901.2455\n",
      "Epoch 337/600\n",
      " - 0s - loss: 750.4121 - val_loss: 902.5400\n",
      "Epoch 338/600\n",
      " - 0s - loss: 750.6251 - val_loss: 903.4420\n",
      "Epoch 339/600\n",
      " - 0s - loss: 750.4230 - val_loss: 903.3304\n",
      "Epoch 340/600\n",
      " - 0s - loss: 749.8035 - val_loss: 902.8861\n",
      "Epoch 341/600\n",
      " - 0s - loss: 750.1306 - val_loss: 903.0807\n",
      "Epoch 342/600\n",
      " - 0s - loss: 750.8911 - val_loss: 901.5647\n",
      "Epoch 343/600\n",
      " - 0s - loss: 751.7108 - val_loss: 899.7991\n",
      "Epoch 344/600\n",
      " - 0s - loss: 750.1422 - val_loss: 898.6450\n",
      "Epoch 345/600\n",
      " - 0s - loss: 749.8820 - val_loss: 896.6152\n",
      "Epoch 346/600\n",
      " - 0s - loss: 749.4368 - val_loss: 894.3602\n",
      "Epoch 347/600\n",
      " - 0s - loss: 748.7011 - val_loss: 894.2177\n",
      "Epoch 348/600\n",
      " - 0s - loss: 750.0678 - val_loss: 894.9027\n",
      "Epoch 349/600\n",
      " - 0s - loss: 750.4531 - val_loss: 895.0288\n",
      "Epoch 350/600\n",
      " - 0s - loss: 749.5712 - val_loss: 894.6944\n",
      "Epoch 351/600\n",
      " - 0s - loss: 748.8761 - val_loss: 894.9498\n",
      "Epoch 352/600\n",
      " - 0s - loss: 749.5110 - val_loss: 896.0679\n",
      "Epoch 353/600\n",
      " - 0s - loss: 749.8785 - val_loss: 897.2362\n",
      "Epoch 354/600\n",
      " - 0s - loss: 750.9391 - val_loss: 898.5883\n",
      "Epoch 355/600\n",
      " - 0s - loss: 749.5598 - val_loss: 898.6943\n",
      "Epoch 356/600\n",
      " - 0s - loss: 749.9790 - val_loss: 897.8296\n",
      "Epoch 357/600\n",
      " - 0s - loss: 749.0794 - val_loss: 896.1886\n",
      "Epoch 358/600\n",
      " - 0s - loss: 749.4050 - val_loss: 893.3638\n",
      "Epoch 359/600\n",
      " - 0s - loss: 750.2410 - val_loss: 890.4494\n",
      "Epoch 360/600\n",
      " - 0s - loss: 747.5854 - val_loss: 888.1057\n",
      "Epoch 361/600\n",
      " - 0s - loss: 748.7940 - val_loss: 887.2497\n",
      "Epoch 362/600\n",
      " - 0s - loss: 749.4578 - val_loss: 887.2203\n",
      "Epoch 363/600\n",
      " - 0s - loss: 750.3044 - val_loss: 889.0603\n",
      "Epoch 364/600\n",
      " - 0s - loss: 748.4391 - val_loss: 891.4615\n",
      "Epoch 365/600\n",
      " - 0s - loss: 749.0197 - val_loss: 893.5790\n",
      "Epoch 366/600\n",
      " - 0s - loss: 749.7820 - val_loss: 895.8794\n",
      "Epoch 367/600\n",
      " - 0s - loss: 748.0321 - val_loss: 897.7752\n",
      "Epoch 368/600\n",
      " - 0s - loss: 749.4031 - val_loss: 897.1684\n",
      "Epoch 369/600\n",
      " - 0s - loss: 748.8326 - val_loss: 894.6707\n",
      "Epoch 370/600\n",
      " - 0s - loss: 750.6901 - val_loss: 892.6864\n",
      "Epoch 371/600\n",
      " - 0s - loss: 748.4211 - val_loss: 890.2249\n",
      "Epoch 372/600\n",
      " - 0s - loss: 748.5130 - val_loss: 889.5230\n",
      "Epoch 373/600\n",
      " - 0s - loss: 748.8533 - val_loss: 890.0831\n",
      "Epoch 374/600\n",
      " - 0s - loss: 749.0413 - val_loss: 890.9214\n",
      "Epoch 375/600\n",
      " - 0s - loss: 749.2369 - val_loss: 893.2833\n",
      "Epoch 376/600\n",
      " - 0s - loss: 747.9871 - val_loss: 897.1331\n",
      "Epoch 377/600\n",
      " - 0s - loss: 750.2623 - val_loss: 899.6120\n",
      "Epoch 378/600\n",
      " - 0s - loss: 751.4335 - val_loss: 898.0631\n",
      "Epoch 379/600\n",
      " - 0s - loss: 749.2216 - val_loss: 895.7376\n",
      "Epoch 380/600\n",
      " - 0s - loss: 748.7690 - val_loss: 891.6344\n",
      "Epoch 381/600\n",
      " - 0s - loss: 748.5232 - val_loss: 888.8225\n",
      "Epoch 382/600\n",
      " - 0s - loss: 748.2032 - val_loss: 889.2114\n",
      "Epoch 383/600\n",
      " - 0s - loss: 748.0733 - val_loss: 893.6288\n",
      "Epoch 384/600\n",
      " - 0s - loss: 748.0079 - val_loss: 900.0654\n",
      "Epoch 385/600\n",
      " - 0s - loss: 749.1073 - val_loss: 906.6580\n",
      "Epoch 386/600\n",
      " - 0s - loss: 750.1205 - val_loss: 911.5239\n",
      "Epoch 387/600\n",
      " - 0s - loss: 748.5153 - val_loss: 914.6634\n",
      "Epoch 388/600\n",
      " - 0s - loss: 748.4255 - val_loss: 914.9473\n",
      "Epoch 389/600\n",
      " - 0s - loss: 749.8305 - val_loss: 914.9229\n",
      "Epoch 390/600\n",
      " - 0s - loss: 748.9777 - val_loss: 913.9087\n",
      "Epoch 391/600\n",
      " - 0s - loss: 748.3757 - val_loss: 911.4791\n",
      "Epoch 392/600\n",
      " - 0s - loss: 749.2349 - val_loss: 907.4385\n",
      "Epoch 393/600\n",
      " - 0s - loss: 748.6335 - val_loss: 903.0563\n",
      "Epoch 394/600\n",
      " - 0s - loss: 748.4279 - val_loss: 898.9378\n",
      "Epoch 395/600\n",
      " - 0s - loss: 748.1388 - val_loss: 894.8769\n",
      "Epoch 396/600\n",
      " - 0s - loss: 747.9011 - val_loss: 892.5015\n",
      "Epoch 397/600\n",
      " - 0s - loss: 748.0724 - val_loss: 891.3048\n",
      "Epoch 398/600\n",
      " - 0s - loss: 748.2286 - val_loss: 891.8457\n",
      "Epoch 399/600\n",
      " - 0s - loss: 748.2848 - val_loss: 893.8893\n",
      "Epoch 400/600\n",
      " - 0s - loss: 749.1448 - val_loss: 898.0314\n",
      "Epoch 401/600\n",
      " - 0s - loss: 748.1492 - val_loss: 903.8539\n",
      "Epoch 402/600\n",
      " - 0s - loss: 748.2747 - val_loss: 907.6937\n",
      "Epoch 403/600\n",
      " - 0s - loss: 748.5654 - val_loss: 908.9841\n",
      "Epoch 404/600\n",
      " - 0s - loss: 748.4622 - val_loss: 906.5607\n",
      "Epoch 405/600\n",
      " - 0s - loss: 747.9450 - val_loss: 903.2156\n",
      "Epoch 406/600\n",
      " - 0s - loss: 747.5950 - val_loss: 901.1972\n",
      "Epoch 407/600\n",
      " - 0s - loss: 748.3588 - val_loss: 899.2100\n",
      "Epoch 408/600\n",
      " - 0s - loss: 747.9333 - val_loss: 897.2701\n",
      "Epoch 409/600\n",
      " - 0s - loss: 749.0690 - val_loss: 896.8036\n",
      "Epoch 410/600\n",
      " - 0s - loss: 747.3763 - val_loss: 897.9120\n",
      "Epoch 411/600\n",
      " - 0s - loss: 747.2531 - val_loss: 899.8331\n",
      "Epoch 412/600\n",
      " - 0s - loss: 748.5132 - val_loss: 901.3552\n",
      "Epoch 413/600\n",
      " - 0s - loss: 747.3257 - val_loss: 902.5165\n",
      "Epoch 414/600\n",
      " - 0s - loss: 749.5994 - val_loss: 901.1043\n",
      "Epoch 415/600\n",
      " - 0s - loss: 748.0728 - val_loss: 897.6389\n",
      "Epoch 416/600\n",
      " - 0s - loss: 747.7950 - val_loss: 894.7370\n",
      "Epoch 417/600\n",
      " - 0s - loss: 748.1129 - val_loss: 891.4580\n",
      "Epoch 418/600\n",
      " - 0s - loss: 747.6329 - val_loss: 889.2996\n",
      "Epoch 419/600\n",
      " - 0s - loss: 747.9103 - val_loss: 888.3990\n",
      "Epoch 420/600\n",
      " - 0s - loss: 748.8995 - val_loss: 888.4073\n",
      "Epoch 421/600\n",
      " - 0s - loss: 747.2757 - val_loss: 890.4092\n",
      "Epoch 422/600\n",
      " - 0s - loss: 748.6074 - val_loss: 892.7831\n",
      "Epoch 423/600\n",
      " - 0s - loss: 747.7063 - val_loss: 895.6326\n",
      "Epoch 424/600\n",
      " - 0s - loss: 748.6229 - val_loss: 899.3847\n",
      "Epoch 425/600\n",
      " - 0s - loss: 747.1545 - val_loss: 901.9107\n",
      "Epoch 426/600\n",
      " - 0s - loss: 746.6819 - val_loss: 902.4993\n",
      "Epoch 427/600\n",
      " - 0s - loss: 747.8109 - val_loss: 901.5925\n",
      "Epoch 428/600\n",
      " - 0s - loss: 747.7491 - val_loss: 898.3917\n",
      "Epoch 429/600\n",
      " - 0s - loss: 746.3376 - val_loss: 894.8371\n",
      "Epoch 430/600\n",
      " - 0s - loss: 746.7649 - val_loss: 891.0120\n",
      "Epoch 431/600\n",
      " - 0s - loss: 748.7643 - val_loss: 888.5497\n",
      "Epoch 432/600\n",
      " - 0s - loss: 747.9916 - val_loss: 888.2042\n",
      "Epoch 433/600\n",
      " - 0s - loss: 747.0240 - val_loss: 888.8737\n",
      "Epoch 434/600\n",
      " - 0s - loss: 746.7612 - val_loss: 890.6351\n",
      "Epoch 435/600\n",
      " - 0s - loss: 748.3607 - val_loss: 893.5161\n",
      "Epoch 436/600\n",
      " - 0s - loss: 746.6009 - val_loss: 896.2251\n",
      "Epoch 437/600\n",
      " - 0s - loss: 748.3975 - val_loss: 897.6029\n",
      "Epoch 438/600\n",
      " - 0s - loss: 746.4554 - val_loss: 897.9534\n",
      "Epoch 439/600\n",
      " - 0s - loss: 746.8003 - val_loss: 897.4662\n",
      "Epoch 440/600\n",
      " - 0s - loss: 747.0975 - val_loss: 895.9455\n",
      "Epoch 441/600\n",
      " - 0s - loss: 747.5862 - val_loss: 893.4210\n",
      "Epoch 442/600\n",
      " - 0s - loss: 746.4572 - val_loss: 891.4090\n",
      "Epoch 443/600\n",
      " - 0s - loss: 746.8199 - val_loss: 890.5088\n",
      "Epoch 444/600\n",
      " - 0s - loss: 747.0811 - val_loss: 889.7701\n",
      "Epoch 445/600\n",
      " - 0s - loss: 748.4685 - val_loss: 890.4880\n",
      "Epoch 446/600\n",
      " - 0s - loss: 746.3394 - val_loss: 891.3762\n",
      "Epoch 447/600\n",
      " - 0s - loss: 748.1086 - val_loss: 893.8058\n",
      "Epoch 448/600\n",
      " - 0s - loss: 746.4042 - val_loss: 896.1930\n",
      "Epoch 449/600\n",
      " - 0s - loss: 747.4224 - val_loss: 898.1834\n",
      "Epoch 450/600\n",
      " - 0s - loss: 747.2828 - val_loss: 898.2775\n",
      "Epoch 451/600\n",
      " - 0s - loss: 747.7141 - val_loss: 897.1236\n",
      "Epoch 452/600\n",
      " - 0s - loss: 746.1166 - val_loss: 895.0188\n",
      "Epoch 453/600\n",
      " - 0s - loss: 747.2364 - val_loss: 892.0642\n",
      "Epoch 454/600\n",
      " - 0s - loss: 747.0690 - val_loss: 888.7178\n",
      "Epoch 455/600\n",
      " - 0s - loss: 746.7679 - val_loss: 886.7043\n",
      "Epoch 456/600\n",
      " - 0s - loss: 746.9527 - val_loss: 885.6364\n",
      "Epoch 457/600\n",
      " - 0s - loss: 747.1552 - val_loss: 885.8459\n",
      "Epoch 458/600\n",
      " - 0s - loss: 747.2119 - val_loss: 887.0905\n",
      "Epoch 459/600\n",
      " - 0s - loss: 746.8798 - val_loss: 888.6165\n",
      "Epoch 460/600\n",
      " - 0s - loss: 745.6616 - val_loss: 889.9605\n",
      "Epoch 461/600\n",
      " - 0s - loss: 746.4122 - val_loss: 891.8262\n",
      "Epoch 462/600\n",
      " - 0s - loss: 746.5499 - val_loss: 892.3181\n",
      "Epoch 463/600\n",
      " - 0s - loss: 747.2297 - val_loss: 891.2366\n",
      "Epoch 464/600\n",
      " - 0s - loss: 747.3881 - val_loss: 889.8944\n",
      "Epoch 465/600\n",
      " - 0s - loss: 746.6419 - val_loss: 888.7673\n",
      "Epoch 466/600\n",
      " - 0s - loss: 747.3376 - val_loss: 888.6354\n",
      "Epoch 467/600\n",
      " - 0s - loss: 745.8320 - val_loss: 888.5924\n",
      "Epoch 468/600\n",
      " - 0s - loss: 747.5639 - val_loss: 888.2512\n",
      "Epoch 469/600\n",
      " - 0s - loss: 747.1312 - val_loss: 887.9775\n",
      "Epoch 470/600\n",
      " - 0s - loss: 746.9902 - val_loss: 886.9090\n",
      "Epoch 471/600\n",
      " - 0s - loss: 746.5811 - val_loss: 886.2993\n",
      "Epoch 472/600\n",
      " - 0s - loss: 746.2205 - val_loss: 886.8288\n",
      "Epoch 473/600\n",
      " - 0s - loss: 747.0572 - val_loss: 888.3940\n",
      "Epoch 474/600\n",
      " - 0s - loss: 746.1572 - val_loss: 890.3381\n",
      "Epoch 475/600\n",
      " - 0s - loss: 746.3452 - val_loss: 892.6505\n",
      "Epoch 476/600\n",
      " - 0s - loss: 746.7576 - val_loss: 894.7886\n",
      "Epoch 477/600\n",
      " - 0s - loss: 747.6384 - val_loss: 896.4089\n",
      "Epoch 478/600\n",
      " - 0s - loss: 747.7793 - val_loss: 895.8883\n",
      "Epoch 479/600\n",
      " - 0s - loss: 746.0928 - val_loss: 895.2956\n",
      "Epoch 480/600\n",
      " - 0s - loss: 746.7480 - val_loss: 893.6605\n",
      "Epoch 481/600\n",
      " - 0s - loss: 746.5581 - val_loss: 891.6780\n",
      "Epoch 482/600\n",
      " - 0s - loss: 745.5330 - val_loss: 890.7834\n",
      "Epoch 483/600\n",
      " - 0s - loss: 745.5319 - val_loss: 889.5493\n",
      "Epoch 484/600\n",
      " - 0s - loss: 745.5884 - val_loss: 888.7635\n",
      "Epoch 485/600\n",
      " - 0s - loss: 746.0318 - val_loss: 888.5688\n",
      "Epoch 486/600\n",
      " - 0s - loss: 745.7422 - val_loss: 888.5115\n",
      "Epoch 487/600\n",
      " - 0s - loss: 746.3844 - val_loss: 889.8235\n",
      "Epoch 488/600\n",
      " - 0s - loss: 745.9031 - val_loss: 890.9422\n",
      "Epoch 489/600\n",
      " - 0s - loss: 746.6971 - val_loss: 892.5401\n",
      "Epoch 490/600\n",
      " - 0s - loss: 745.6357 - val_loss: 894.0174\n",
      "Epoch 491/600\n",
      " - 0s - loss: 746.9266 - val_loss: 895.2908\n",
      "Epoch 492/600\n",
      " - 0s - loss: 745.8110 - val_loss: 896.0426\n",
      "Epoch 493/600\n",
      " - 0s - loss: 746.8279 - val_loss: 895.5209\n",
      "Epoch 494/600\n",
      " - 0s - loss: 746.3353 - val_loss: 894.9056\n",
      "Epoch 495/600\n",
      " - 0s - loss: 747.3325 - val_loss: 893.9343\n",
      "Epoch 496/600\n",
      " - 0s - loss: 746.6916 - val_loss: 893.7350\n",
      "Epoch 497/600\n",
      " - 0s - loss: 745.4805 - val_loss: 894.2592\n",
      "Epoch 498/600\n",
      " - 0s - loss: 746.0417 - val_loss: 893.8348\n",
      "Epoch 499/600\n",
      " - 0s - loss: 746.6697 - val_loss: 892.6321\n",
      "Epoch 500/600\n",
      " - 0s - loss: 747.1902 - val_loss: 890.0168\n",
      "Epoch 501/600\n",
      " - 0s - loss: 746.5193 - val_loss: 887.1501\n",
      "Epoch 502/600\n",
      " - 0s - loss: 745.1097 - val_loss: 884.4697\n",
      "Epoch 503/600\n",
      " - 0s - loss: 745.2263 - val_loss: 883.5611\n",
      "Epoch 504/600\n",
      " - 0s - loss: 745.8460 - val_loss: 884.5354\n",
      "Epoch 505/600\n",
      " - 0s - loss: 745.9079 - val_loss: 886.2636\n",
      "Epoch 506/600\n",
      " - 0s - loss: 746.0953 - val_loss: 888.7606\n",
      "Epoch 507/600\n",
      " - 0s - loss: 745.6778 - val_loss: 890.8611\n",
      "Epoch 508/600\n",
      " - 0s - loss: 744.6398 - val_loss: 893.1393\n",
      "Epoch 509/600\n",
      " - 0s - loss: 745.5157 - val_loss: 893.9099\n",
      "Epoch 510/600\n",
      " - 0s - loss: 745.3998 - val_loss: 894.5298\n",
      "Epoch 511/600\n",
      " - 0s - loss: 746.1270 - val_loss: 894.1996\n",
      "Epoch 512/600\n",
      " - 0s - loss: 746.5749 - val_loss: 893.0864\n",
      "Epoch 513/600\n",
      " - 0s - loss: 746.4802 - val_loss: 893.4629\n",
      "Epoch 514/600\n",
      " - 0s - loss: 744.9290 - val_loss: 894.7324\n",
      "Epoch 515/600\n",
      " - 0s - loss: 745.6644 - val_loss: 896.5098\n",
      "Epoch 516/600\n",
      " - 0s - loss: 746.7243 - val_loss: 896.3820\n",
      "Epoch 517/600\n",
      " - 0s - loss: 747.0270 - val_loss: 896.0669\n",
      "Epoch 518/600\n",
      " - 0s - loss: 746.1605 - val_loss: 895.3230\n",
      "Epoch 519/600\n",
      " - 0s - loss: 745.9405 - val_loss: 893.9756\n",
      "Epoch 520/600\n",
      " - 0s - loss: 745.4090 - val_loss: 892.6666\n",
      "Epoch 521/600\n",
      " - 0s - loss: 745.7613 - val_loss: 891.1672\n",
      "Epoch 522/600\n",
      " - 0s - loss: 745.2115 - val_loss: 889.1207\n",
      "Epoch 523/600\n",
      " - 0s - loss: 746.9135 - val_loss: 887.5812\n",
      "Epoch 524/600\n",
      " - 0s - loss: 746.1907 - val_loss: 887.8981\n",
      "Epoch 525/600\n",
      " - 0s - loss: 745.9501 - val_loss: 888.2278\n",
      "Epoch 526/600\n",
      " - 0s - loss: 745.5867 - val_loss: 889.6796\n",
      "Epoch 527/600\n",
      " - 0s - loss: 745.2365 - val_loss: 890.7493\n",
      "Epoch 528/600\n",
      " - 0s - loss: 745.7239 - val_loss: 892.4523\n",
      "Epoch 529/600\n",
      " - 0s - loss: 747.2907 - val_loss: 894.1943\n",
      "Epoch 530/600\n",
      " - 0s - loss: 745.3438 - val_loss: 896.1216\n",
      "Epoch 531/600\n",
      " - 0s - loss: 745.6625 - val_loss: 897.3837\n",
      "Epoch 532/600\n",
      " - 0s - loss: 745.6661 - val_loss: 897.5635\n",
      "Epoch 533/600\n",
      " - 0s - loss: 745.6903 - val_loss: 897.3261\n",
      "Epoch 534/600\n",
      " - 0s - loss: 744.6081 - val_loss: 896.4543\n",
      "Epoch 535/600\n",
      " - 0s - loss: 745.7812 - val_loss: 894.6607\n",
      "Epoch 536/600\n",
      " - 0s - loss: 746.0616 - val_loss: 892.0590\n",
      "Epoch 537/600\n",
      " - 0s - loss: 745.4539 - val_loss: 890.8036\n",
      "Epoch 538/600\n",
      " - 0s - loss: 745.0291 - val_loss: 889.9140\n",
      "Epoch 539/600\n",
      " - 0s - loss: 746.3347 - val_loss: 889.2904\n",
      "Epoch 540/600\n",
      " - 0s - loss: 745.2643 - val_loss: 889.4758\n",
      "Epoch 541/600\n",
      " - 0s - loss: 744.9576 - val_loss: 889.8163\n",
      "Epoch 542/600\n",
      " - 0s - loss: 745.3890 - val_loss: 890.0716\n",
      "Epoch 543/600\n",
      " - 0s - loss: 746.0443 - val_loss: 889.3382\n",
      "Epoch 544/600\n",
      " - 0s - loss: 744.9542 - val_loss: 889.3469\n",
      "Epoch 545/600\n",
      " - 0s - loss: 745.5335 - val_loss: 889.1338\n",
      "Epoch 546/600\n",
      " - 0s - loss: 745.1788 - val_loss: 890.4649\n",
      "Epoch 547/600\n",
      " - 0s - loss: 745.1543 - val_loss: 892.1725\n",
      "Epoch 548/600\n",
      " - 0s - loss: 744.0518 - val_loss: 894.1931\n",
      "Epoch 549/600\n",
      " - 0s - loss: 744.3787 - val_loss: 895.4408\n",
      "Epoch 550/600\n",
      " - 0s - loss: 743.5931 - val_loss: 896.3458\n",
      "Epoch 551/600\n",
      " - 0s - loss: 745.0590 - val_loss: 895.4749\n",
      "Epoch 552/600\n",
      " - 0s - loss: 745.2518 - val_loss: 893.6763\n",
      "Epoch 553/600\n",
      " - 0s - loss: 745.0383 - val_loss: 891.6483\n",
      "Epoch 554/600\n",
      " - 0s - loss: 744.6256 - val_loss: 889.6939\n",
      "Epoch 555/600\n",
      " - 0s - loss: 744.6888 - val_loss: 888.8140\n",
      "Epoch 556/600\n",
      " - 0s - loss: 745.0541 - val_loss: 887.9215\n",
      "Epoch 557/600\n",
      " - 0s - loss: 744.3809 - val_loss: 887.3944\n",
      "Epoch 558/600\n",
      " - 0s - loss: 744.7137 - val_loss: 887.7483\n",
      "Epoch 559/600\n",
      " - 0s - loss: 745.4216 - val_loss: 888.2457\n",
      "Epoch 560/600\n",
      " - 0s - loss: 745.0046 - val_loss: 887.8434\n",
      "Epoch 561/600\n",
      " - 0s - loss: 745.0912 - val_loss: 886.6392\n",
      "Epoch 562/600\n",
      " - 0s - loss: 746.0338 - val_loss: 886.3314\n",
      "Epoch 563/600\n",
      " - 0s - loss: 745.3558 - val_loss: 886.4843\n",
      "Epoch 564/600\n",
      " - 0s - loss: 745.2542 - val_loss: 886.7304\n",
      "Epoch 565/600\n",
      " - 0s - loss: 744.5716 - val_loss: 887.6212\n",
      "Epoch 566/600\n",
      " - 0s - loss: 744.9210 - val_loss: 887.9182\n",
      "Epoch 567/600\n",
      " - 0s - loss: 744.8983 - val_loss: 888.0226\n",
      "Epoch 568/600\n",
      " - 0s - loss: 744.8845 - val_loss: 887.6614\n",
      "Epoch 569/600\n",
      " - 0s - loss: 744.7653 - val_loss: 886.7459\n",
      "Epoch 570/600\n",
      " - 0s - loss: 744.5795 - val_loss: 886.1289\n",
      "Epoch 571/600\n",
      " - 0s - loss: 744.5270 - val_loss: 885.8730\n",
      "Epoch 572/600\n",
      " - 0s - loss: 745.0391 - val_loss: 886.0144\n",
      "Epoch 573/600\n",
      " - 0s - loss: 746.3267 - val_loss: 886.6278\n",
      "Epoch 574/600\n",
      " - 0s - loss: 744.7062 - val_loss: 887.9212\n",
      "Epoch 575/600\n",
      " - 0s - loss: 745.0384 - val_loss: 889.6990\n",
      "Epoch 576/600\n",
      " - 0s - loss: 744.6571 - val_loss: 890.9468\n",
      "Epoch 577/600\n",
      " - 0s - loss: 745.7733 - val_loss: 891.4978\n",
      "Epoch 578/600\n",
      " - 0s - loss: 744.7781 - val_loss: 891.9535\n",
      "Epoch 579/600\n",
      " - 0s - loss: 744.0350 - val_loss: 892.6154\n",
      "Epoch 580/600\n",
      " - 0s - loss: 745.4504 - val_loss: 893.0595\n",
      "Epoch 581/600\n",
      " - 0s - loss: 744.6172 - val_loss: 892.5331\n",
      "Epoch 582/600\n",
      " - 0s - loss: 745.6442 - val_loss: 891.5635\n",
      "Epoch 583/600\n",
      " - 0s - loss: 744.3054 - val_loss: 890.1932\n",
      "Epoch 584/600\n",
      " - 0s - loss: 744.7737 - val_loss: 891.0364\n",
      "Epoch 585/600\n",
      " - 0s - loss: 744.3785 - val_loss: 892.4017\n",
      "Epoch 586/600\n",
      " - 0s - loss: 745.4918 - val_loss: 892.8670\n",
      "Epoch 587/600\n",
      " - 0s - loss: 744.0751 - val_loss: 893.9813\n",
      "Epoch 588/600\n",
      " - 0s - loss: 744.8622 - val_loss: 894.6921\n",
      "Epoch 589/600\n",
      " - 0s - loss: 743.5345 - val_loss: 895.4961\n",
      "Epoch 590/600\n",
      " - 0s - loss: 744.6127 - val_loss: 895.9956\n",
      "Epoch 591/600\n",
      " - 0s - loss: 746.2162 - val_loss: 894.3120\n",
      "Epoch 592/600\n",
      " - 0s - loss: 744.2769 - val_loss: 893.2800\n",
      "Epoch 593/600\n",
      " - 0s - loss: 744.7422 - val_loss: 892.4307\n",
      "Epoch 594/600\n",
      " - 0s - loss: 743.7900 - val_loss: 892.3805\n",
      "Epoch 595/600\n",
      " - 0s - loss: 745.4669 - val_loss: 892.6342\n",
      "Epoch 596/600\n",
      " - 0s - loss: 744.8020 - val_loss: 893.2604\n",
      "Epoch 597/600\n",
      " - 0s - loss: 745.6354 - val_loss: 894.1348\n",
      "Epoch 598/600\n",
      " - 0s - loss: 744.5987 - val_loss: 894.8502\n",
      "Epoch 599/600\n",
      " - 0s - loss: 744.3835 - val_loss: 896.1088\n",
      "Epoch 600/600\n",
      " - 0s - loss: 744.3129 - val_loss: 896.6095\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2275d660608>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model_pre = finish_model.predict([image_test_target,day_test_target])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#transfer with DAN 预测精度计算\r\n",
    "\r\n",
    "mape_mean = mape_loss_func(model_pre, label_test_target)\r\n",
    "smape_mean = smape_loss_func(model_pre, label_test_target)\r\n",
    "mae_mean = mae_loss_func(model_pre, label_test_target)\r\n",
    "\r\n",
    "print('mape = ' + str(mape_mean) + '\\n' + 'smape = ' + str(smape_mean) + '\\n' + 'mae = ' + str(mae_mean))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mape = 0.18426038573568623\n",
      "smape = 0.17835187066308672\n",
      "mae = 9.409976378659644\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "mape_list = []\r\n",
    "for i in range(num_links):\r\n",
    "    a1 = mape_loss_func(model_pre[:,i,:], label_test_target[:,i,:])\r\n",
    "    mape_list.append(a1)\r\n",
    "\r\n",
    "mape_pd = pd.Series(mape_list)\r\n",
    "mape_pd.sort_values()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "71    0.048404\n",
       "77    0.074991\n",
       "45    0.084538\n",
       "49    0.085888\n",
       "64    0.086190\n",
       "        ...   \n",
       "3     0.311950\n",
       "11    0.318855\n",
       "69    0.413262\n",
       "20    0.449381\n",
       "17    0.897117\n",
       "Length: 82, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "6b092d8890fbfc1935d95d43d0881a7b3742c06492f450993a24f5c2e6237594"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}